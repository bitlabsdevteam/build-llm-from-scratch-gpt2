{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# LLM Pre-training Dataset Preparation\n",
        "## Load, Merge, and Split Dataset for Tokenization\n",
        "\n",
        "This notebook provides optimized methods for:\n",
        "1. Loading text from local .txt files\n",
        "2. Loading datasets from HuggingFace Hub (with proper streaming support)\n",
        "3. Merging multiple data sources\n",
        "4. Creating input-target pairs for causal language modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install_cell"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers torch accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tiktoken > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "yDj-4w6vQfff"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Weights and Biases"
      ],
      "metadata": {
        "id": "9LJdmbevXNpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -q"
      ],
      "metadata": {
        "id": "ERWgR5DbXM8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODEL CONFIGURATION"
      ],
      "metadata": {
        "id": "YIOWfZKaJr99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 128, # Context length\n",
        "    \"emb_dim\": 64,         # Embedding dimension\n",
        "    \"n_heads\": 8,          # Number of attention heads\n",
        "    \"n_layers\": 8,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False,       # Query-Key-Value bias\n",
        "    \"max_length\": 128,      # Maximum sequence length\n",
        "    \"output_dimension\": 64, # Output dimension\n",
        "    \"batch_size\": 2        # batch size\n",
        "}"
      ],
      "metadata": {
        "id": "sup0ja7CKyW6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "import_cell"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
        "from typing import List, Dict, Optional, Union, Tuple\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjBbHxtuQjWY",
        "outputId": "a9865dd5-2cc4-412d-ba04-a1d2fe67ca9b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialise W&B"
      ],
      "metadata": {
        "id": "DiwZu-3LYjXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Weights & Biases\n",
        "def init_wandb(config, project_name=\"gpt-pretraining\"):\n",
        "    \"\"\"Initialize W&B tracking\"\"\"\n",
        "    wandb.init(\n",
        "        project=project_name,\n",
        "        config=config,\n",
        "        name=f\"gpt-{config['num_epochs']}ep-lr{config['learning_rate']}\",\n",
        "        tags=[\"gpt\", \"pretraining\", \"custom\"]\n",
        "    )\n",
        "    wandb.watch(model, log=\"all\", log_freq=100)  # Log gradients and parameters"
      ],
      "metadata": {
        "id": "tUf8tN_nYmGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_txt"
      },
      "source": [
        "## 1. Load Text from Local .txt Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "load_txt_cell"
      },
      "outputs": [],
      "source": [
        "def load_txt_file(\n",
        "    file_path: str,\n",
        "    encoding: str = 'utf-8',\n",
        "    chunk_size: Optional[int] = None,\n",
        "    overlap: int = 0\n",
        ") -> Dataset:\n",
        "    \"\"\"\n",
        "    Load text data from a .txt file and convert to HuggingFace Dataset.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the .txt file\n",
        "        encoding: Text encoding (default: 'utf-8')\n",
        "        chunk_size: Optional size to split text into chunks (in characters)\n",
        "        overlap: Number of overlapping characters between chunks\n",
        "\n",
        "    Returns:\n",
        "        HuggingFace Dataset containing text data\n",
        "    \"\"\"\n",
        "    print(f\"Loading text from: {file_path}\")\n",
        "\n",
        "    # Read the file\n",
        "    with open(file_path, 'r', encoding=encoding) as f:\n",
        "        text_content = f.read()\n",
        "\n",
        "    # Split into chunks if specified\n",
        "    if chunk_size:\n",
        "        texts = []\n",
        "        start = 0\n",
        "        while start < len(text_content):\n",
        "            end = start + chunk_size\n",
        "            texts.append(text_content[start:end])\n",
        "            start = end - overlap\n",
        "    else:\n",
        "        # Split by paragraphs (double newline) or keep as single text\n",
        "        texts = [t.strip() for t in text_content.split('\\n\\n') if t.strip()]\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = Dataset.from_dict({\"text\": texts})\n",
        "\n",
        "    print(f\"✓ Loaded {len(dataset):,} text samples from .txt file\")\n",
        "    print(f\"Total characters: {sum(len(t) for t in texts):,}\")\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_hf"
      },
      "source": [
        "## 2. Load Dataset from HuggingFace Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "load_hf_cell"
      },
      "outputs": [],
      "source": [
        "def load_huggingface_dataset(\n",
        "    dataset_name: str,\n",
        "    text_column: str = \"text\",\n",
        "    split: str = \"train\",\n",
        "    name: Optional[str] = None,\n",
        "    num_samples: Optional[int] = None,\n",
        "    streaming: bool = True,\n",
        "    trust_remote_code: bool = False\n",
        ") -> Dataset:\n",
        "    \"\"\"\n",
        "    Load dataset from HuggingFace Hub with optimized streaming support.\n",
        "\n",
        "    Args:\n",
        "        dataset_name: Name of the dataset on HuggingFace Hub\n",
        "                     (e.g., 'HuggingFaceFW/fineweb', 'openwebtext')\n",
        "        text_column: Name of the column containing text data (default: 'text')\n",
        "        split: Dataset split to load (default: 'train')\n",
        "        name: Dataset configuration name (e.g., 'sample-10BT' for fineweb)\n",
        "        num_samples: Limit number of samples to load (recommended for large datasets)\n",
        "        streaming: Use streaming mode for memory efficiency (default: True)\n",
        "        trust_remote_code: Trust remote code for custom datasets\n",
        "\n",
        "    Returns:\n",
        "        HuggingFace Dataset with 'text' column\n",
        "\n",
        "    Examples:\n",
        "        # Load FineWeb dataset\n",
        "        dataset = load_huggingface_dataset(\n",
        "            dataset_name=\"HuggingFaceFW/fineweb\",\n",
        "            name=\"sample-10BT\",\n",
        "            num_samples=10000\n",
        "        )\n",
        "\n",
        "        # Load OpenWebText\n",
        "        dataset = load_huggingface_dataset(\n",
        "            dataset_name=\"openwebtext\",\n",
        "            num_samples=5000\n",
        "        )\n",
        "    \"\"\"\n",
        "    print(f\"\\nLoading HuggingFace dataset: '{dataset_name}'\" +\n",
        "          (f\" (config: {name})\" if name else \"\") +\n",
        "          f\" (split: {split})\")\n",
        "\n",
        "    try:\n",
        "        # Load dataset with streaming for memory efficiency\n",
        "        dataset = load_dataset(\n",
        "            dataset_name,\n",
        "            name=name,\n",
        "            split=split,\n",
        "            streaming=streaming,\n",
        "            trust_remote_code=trust_remote_code\n",
        "        )\n",
        "\n",
        "        # Extract text from samples\n",
        "        texts = []\n",
        "\n",
        "        if streaming:\n",
        "            # Iterate through streaming dataset (memory efficient)\n",
        "            print(f\"Extracting text from streaming dataset...\")\n",
        "            for i, sample in enumerate(dataset):\n",
        "                if num_samples and i >= num_samples:\n",
        "                    break\n",
        "\n",
        "                # Extract text from the specified column\n",
        "                if text_column in sample:\n",
        "                    texts.append(sample[text_column])\n",
        "                else:\n",
        "                    available = list(sample.keys())\n",
        "                    raise KeyError(\n",
        "                        f\"Column '{text_column}' not found. \"\n",
        "                        f\"Available columns: {available}\"\n",
        "                    )\n",
        "\n",
        "                # Progress indicator\n",
        "                if (i + 1) % 1000 == 0:\n",
        "                    print(f\"  Processed {i + 1:,} samples...\", end=\"\\r\")\n",
        "\n",
        "            if texts:\n",
        "                print(f\"\\n✓ Extracted {len(texts):,} samples from streaming dataset\")\n",
        "        else:\n",
        "            # Non-streaming mode (loads entire dataset into memory)\n",
        "            print(f\"Loading in non-streaming mode...\")\n",
        "            if num_samples:\n",
        "                dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
        "\n",
        "            # Extract text column\n",
        "            if text_column in dataset.column_names:\n",
        "                texts = dataset[text_column]\n",
        "            else:\n",
        "                raise KeyError(\n",
        "                    f\"Column '{text_column}' not found. \"\n",
        "                    f\"Available columns: {dataset.column_names}\"\n",
        "                )\n",
        "\n",
        "            print(f\"✓ Loaded {len(texts):,} samples\")\n",
        "\n",
        "        # Create a new Dataset with only the text column\n",
        "        final_dataset = Dataset.from_dict({\"text\": texts})\n",
        "\n",
        "        total_chars = sum(len(t) for t in texts)\n",
        "        print(f\"Total characters: {total_chars:,}\")\n",
        "        print(f\"Average text length: {total_chars / len(texts):.0f} chars per sample\")\n",
        "\n",
        "        return final_dataset\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading dataset: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "merge"
      },
      "source": [
        "## 3. Merge Multiple Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "merge_cell"
      },
      "outputs": [],
      "source": [
        "def merge_datasets(\n",
        "    datasets: List[Dataset],\n",
        "    shuffle: bool = True,\n",
        "    seed: int = 42,\n",
        "    interleave: bool = False\n",
        ") -> Dataset:\n",
        "    \"\"\"\n",
        "    Merge multiple datasets into a single dataset.\n",
        "\n",
        "    Args:\n",
        "        datasets: List of HuggingFace Datasets to merge\n",
        "        shuffle: Whether to shuffle the merged dataset\n",
        "        seed: Random seed for shuffling\n",
        "        interleave: If True, interleave datasets instead of concatenating\n",
        "\n",
        "    Returns:\n",
        "        Merged HuggingFace Dataset\n",
        "    \"\"\"\n",
        "    print(f\"\\nMerging {len(datasets)} datasets...\")\n",
        "\n",
        "    # Validate all datasets have 'text' column\n",
        "    for i, ds in enumerate(datasets):\n",
        "        if 'text' not in ds.column_names:\n",
        "            raise ValueError(f\"Dataset {i} does not have 'text' column\")\n",
        "        print(f\"  Dataset {i+1}: {len(ds):,} samples\")\n",
        "\n",
        "    # Merge datasets\n",
        "    if interleave:\n",
        "        # Interleave datasets (useful for balanced sampling)\n",
        "        from datasets import interleave_datasets\n",
        "        merged_dataset = interleave_datasets(datasets, seed=seed)\n",
        "        print(\"Using interleave strategy...\")\n",
        "    else:\n",
        "        # Concatenate datasets\n",
        "        merged_dataset = concatenate_datasets(datasets)\n",
        "        print(\"Using concatenation strategy...\")\n",
        "\n",
        "    # Shuffle if requested\n",
        "    if shuffle:\n",
        "        print(\"Shuffling merged dataset...\")\n",
        "        merged_dataset = merged_dataset.shuffle(seed=seed)\n",
        "\n",
        "    print(f\"✓ Merged dataset contains {len(merged_dataset):,} total samples\")\n",
        "    return merged_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "split"
      },
      "source": [
        "## 4. Create Input-Target Pairs for Pre-training\\n\\n**Two Approaches Available:**\\n1. **Implicit Shifting** (HuggingFace standard) - labels = input_ids\\n2. **Explicit Shifting** (Custom training) - labels = input_ids shifted by 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "split_implicit"
      },
      "source": [
        "### 4.1 Implicit Shifting (HuggingFace Style - Recommended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "split_cell"
      },
      "outputs": [],
      "source": [
        "# def create_input_target_pairs(\n",
        "#     dataset: Dataset,\n",
        "#     tokenizer: AutoTokenizer,\n",
        "#     max_length: int = 512,\n",
        "#     stride: Optional[int] = None,\n",
        "#     preprocessing_num_workers: int = 4,\n",
        "#     batch_size: int = 1000\n",
        "# ) -> Dataset:\n",
        "#     \"\"\"\n",
        "#     Create input-target pairs for causal language modeling (IMPLICIT SHIFTING).\n",
        "\n",
        "#     This is the HuggingFace standard approach where:\n",
        "#     - input_ids = [token_0, token_1, token_2, ..., token_n]\n",
        "#     - labels = [token_0, token_1, token_2, ..., token_n] (SAME as input_ids)\n",
        "\n",
        "#     The model shifts internally during loss calculation:\n",
        "#     - At position i, the model uses input[0:i] to predict label[i]\n",
        "\n",
        "#     ✅ Use this for: HuggingFace Transformers, Trainer API, pretrained models\n",
        "\n",
        "#     Args:\n",
        "#         dataset: HuggingFace Dataset with 'text' column\n",
        "#         tokenizer: HuggingFace tokenizer\n",
        "#         max_length: Maximum sequence length\n",
        "#         stride: Stride for sliding window (None = max_length // 2)\n",
        "#         preprocessing_num_workers: Number of parallel workers\n",
        "#         batch_size: Batch size for processing\n",
        "\n",
        "#     Returns:\n",
        "#         Dataset with 'input_ids', 'attention_mask', and 'labels' columns\n",
        "#     \"\"\"\n",
        "#     print(f\"\\nCreating input-target pairs (IMPLICIT SHIFTING - HuggingFace style)...\")\n",
        "#     print(f\"Max length: {max_length} tokens\")\n",
        "\n",
        "#     # Set stride (overlap) - default to half of max_length for context preservation\n",
        "#     if stride is None:\n",
        "#         stride = max_length // 2\n",
        "#     print(f\"Stride: {stride} tokens (overlap for longer texts)\")\n",
        "\n",
        "#     def tokenize_function(examples):\n",
        "#         \"\"\"\n",
        "#         Tokenize text and create input-target pairs.\n",
        "#         Uses efficient batched tokenization with sliding window.\n",
        "#         \"\"\"\n",
        "#         # Tokenize with return_overflowing_tokens for chunking long texts\n",
        "#         tokenized = tokenizer(\n",
        "#             examples[\"text\"],\n",
        "#             truncation=True,\n",
        "#             max_length=max_length,\n",
        "#             stride=stride,\n",
        "#             return_overflowing_tokens=True,\n",
        "#             return_length=True,\n",
        "#             padding=False,  # We'll pad during batching in training\n",
        "#         )\n",
        "\n",
        "#         # Create labels (same as input_ids for causal LM with HuggingFace)\n",
        "#         # The model will shift internally during training\n",
        "#         tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "\n",
        "#         return tokenized\n",
        "\n",
        "#     # Apply tokenization with multiprocessing\n",
        "#     print(\"Tokenizing dataset...\")\n",
        "#     tokenized_dataset = dataset.map(\n",
        "#         tokenize_function,\n",
        "#         batched=True,\n",
        "#         num_proc=preprocessing_num_workers,\n",
        "#         remove_columns=dataset.column_names,\n",
        "#         batch_size=batch_size,\n",
        "#         desc=\"Tokenizing and creating pairs\"\n",
        "#     )\n",
        "\n",
        "#     # Filter out sequences that are too short\n",
        "#     min_length = 10  # Minimum viable sequence length\n",
        "#     print(f\"Filtering sequences shorter than {min_length} tokens...\")\n",
        "#     tokenized_dataset = tokenized_dataset.filter(\n",
        "#         lambda x: len(x[\"input_ids\"]) >= min_length,\n",
        "#         num_proc=preprocessing_num_workers,\n",
        "#         desc=\"Filtering short sequences\"\n",
        "#     )\n",
        "\n",
        "#     # Calculate statistics\n",
        "#     lengths = [len(x) for x in tokenized_dataset['input_ids']]\n",
        "#     avg_length = np.mean(lengths)\n",
        "\n",
        "#     print(f\"\\n✓ Created {len(tokenized_dataset):,} input-target pairs\")\n",
        "#     print(f\"Average sequence length: {avg_length:.1f} tokens\")\n",
        "#     print(f\"Total tokens: {sum(lengths):,}\")\n",
        "\n",
        "#     return tokenized_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "split_explicit"
      },
      "source": [
        "### 4.2 Explicit Shifting (Custom Training Style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "explicit_cell"
      },
      "outputs": [],
      "source": [
        "def create_input_target_pairs_explicit(\n",
        "    dataset: Dataset,\n",
        "    tokenizer,\n",
        "    max_length: int = 512,\n",
        "    stride: Optional[int] = None,\n",
        "    preprocessing_num_workers: int = 1,  # DEFAULT to 1 (safer)\n",
        "    batch_size: int = 100  # REDUCED default\n",
        ") -> Dataset:\n",
        "    \"\"\"\n",
        "    Create input-target pairs for causal language modeling (EXPLICIT SHIFTING).\n",
        "    \"\"\"\n",
        "    print(f\"\\nCreating input-target pairs (EXPLICIT SHIFTING - Custom style)...\")\n",
        "    print(f\"Max length: {max_length} tokens\")\n",
        "\n",
        "    if stride is None:\n",
        "        stride = max_length\n",
        "    print(f\"Stride: {stride} tokens\")\n",
        "\n",
        "    # Warning for large datasets with multiprocessing\n",
        "    if len(dataset) > 10000 and preprocessing_num_workers > 1:\n",
        "        print(f\"⚠️  Large dataset ({len(dataset):,} samples) with multiprocessing may cause OOM\")\n",
        "        print(f\"   Recommend: preprocessing_num_workers=1\")\n",
        "\n",
        "    def tokenize_and_shift(examples):\n",
        "        \"\"\"\n",
        "        Tokenize text and create explicitly shifted input-target pairs.\n",
        "        \"\"\"\n",
        "        all_input_ids = []\n",
        "        all_labels = []\n",
        "        all_attention_mask = []\n",
        "\n",
        "        try:\n",
        "            for text in examples[\"text\"]:\n",
        "                # Skip empty texts\n",
        "                if not text or len(text.strip()) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Tokenize with tiktoken\n",
        "                try:\n",
        "                    token_ids = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Tokenization failed for text, skipping: {str(e)[:100]}\")\n",
        "                    continue\n",
        "\n",
        "                # Truncate if too long\n",
        "                if len(token_ids) > 1024:\n",
        "                    token_ids = token_ids[:1024]\n",
        "\n",
        "                # Skip if too short\n",
        "                if len(token_ids) < max_length + 1:\n",
        "                    continue\n",
        "\n",
        "                # Create sliding window chunks\n",
        "                for i in range(0, len(token_ids) - max_length, stride):\n",
        "                    input_chunk = token_ids[i : i + max_length]\n",
        "                    target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
        "\n",
        "                    # Only add if we have complete sequences\n",
        "                    if len(input_chunk) == max_length and len(target_chunk) == max_length:\n",
        "                        all_input_ids.append(input_chunk)\n",
        "                        all_labels.append(target_chunk)\n",
        "                        all_attention_mask.append([1] * max_length)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in tokenize_and_shift: {e}\")\n",
        "            # Return empty to avoid crashing\n",
        "            return {\n",
        "                \"input_ids\": [],\n",
        "                \"labels\": [],\n",
        "                \"attention_mask\": []\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": all_input_ids,\n",
        "            \"labels\": all_labels,\n",
        "            \"attention_mask\": all_attention_mask\n",
        "        }\n",
        "\n",
        "    # Apply tokenization\n",
        "    print(\"Tokenizing and shifting dataset...\")\n",
        "    print(f\"Using {preprocessing_num_workers} worker(s)\")\n",
        "\n",
        "    try:\n",
        "        tokenized_dataset = dataset.map(\n",
        "            tokenize_and_shift,\n",
        "            batched=True,\n",
        "            num_proc=preprocessing_num_workers if preprocessing_num_workers > 1 else None,\n",
        "            remove_columns=dataset.column_names,\n",
        "            batch_size=batch_size,\n",
        "            desc=\"Tokenizing and shifting\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Error during tokenization: {e}\")\n",
        "        print(\"Retrying with num_proc=1 (no multiprocessing)...\")\n",
        "\n",
        "        # Retry without multiprocessing\n",
        "        tokenized_dataset = dataset.map(\n",
        "            tokenize_and_shift,\n",
        "            batched=True,\n",
        "            num_proc=None,  # Disable multiprocessing\n",
        "            remove_columns=dataset.column_names,\n",
        "            batch_size=batch_size,\n",
        "            desc=\"Tokenizing and shifting (retry)\"\n",
        "        )\n",
        "\n",
        "    # Filter out empty sequences\n",
        "    original_len = len(tokenized_dataset)\n",
        "    tokenized_dataset = tokenized_dataset.filter(\n",
        "        lambda x: len(x['input_ids']) > 0,\n",
        "        desc=\"Filtering empty sequences\"\n",
        "    )\n",
        "\n",
        "    if len(tokenized_dataset) < original_len:\n",
        "        print(f\"Filtered out {original_len - len(tokenized_dataset)} empty sequences\")\n",
        "\n",
        "    # Calculate statistics\n",
        "    if len(tokenized_dataset) > 0:\n",
        "        print(f\"\\n✓ Created {len(tokenized_dataset):,} input-target pairs\")\n",
        "        print(f\"Sequence length: {max_length} tokens (fixed)\")\n",
        "        print(f\"Total tokens: {len(tokenized_dataset) * max_length:,}\")\n",
        "    else:\n",
        "        print(\"\\n⚠️  Warning: No sequences created! Check your data and max_length\")\n",
        "\n",
        "    return tokenized_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOAD, MERGE AND CREATING TOKEN EMBEDDINGS"
      ],
      "metadata": {
        "id": "dlJtkUtlAOUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# Load tokenizer from transformers\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "#Load tokenizer from tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Step 1: Load raw datasets (text only, no tokenization yet)\n",
        "txt_dataset = load_txt_file(\"the-verdict.txt\")\n",
        "\n",
        "hf_dataset = load_huggingface_dataset(\n",
        "    dataset_name=\"HuggingFaceFW/fineweb\",\n",
        "    name=\"sample-10BT\",\n",
        "    num_samples=5000,\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# Step 2: Merge raw datasets (still have 'text' column)\n",
        "merged_dataset = merge_datasets(\n",
        "    datasets=[txt_dataset, hf_dataset],\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Step 3: NOW apply explicit shifting tokenization\n",
        "explicit_dataset = create_input_target_pairs_explicit(\n",
        "    dataset=merged_dataset,  # Raw text dataset\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=GPT_CONFIG_124M['max_length'],\n",
        "    stride=4\n",
        ")\n",
        "\n",
        "# Create PyTorch DataLoader\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Convert batch to tensors\"\"\"\n",
        "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
        "    labels = torch.tensor([item['labels'] for item in batch])\n",
        "    return input_ids, labels\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    explicit_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Iterate through batches\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(f\"Inputs shape:  {inputs.shape}\")\n",
        "print(f\"Targets shape: {targets.shape}\")\n",
        "print(f\"\\nInputs:\\n{inputs}\")\n",
        "print(f\"\\nTargets:\\n{targets}\")\n",
        "\n",
        "# Create token embeddings\n",
        "token_embedding_layer = torch.nn.Embedding(GPT_CONFIG_124M['vocab_size'], GPT_CONFIG_124M['emb_dim'])\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(f\"\\nToken embeddings shape: {token_embeddings.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 978,
          "referenced_widgets": [
            "f63d0f7b7cee427c80ea9ab26504c388",
            "8bfcd604d3f242269a1d726bc23c51c7",
            "a7832841b8284ff2bfd0011bd05973f6",
            "52614714196d46aa82071afd69bf6cb0",
            "bb1c863a23c446d48aa8409fa5776a32",
            "84fd5aa2ac994f31b3c139ee0f8a8c28",
            "01dce4ddcc624d0d9a02ce6b80a34c9c",
            "9abb04fad9d34e17a9690b774b29c845",
            "2a367c8b856c4c13b62fffe28624f5d7",
            "1400f19bd79744a68cde1bcf4f171b25",
            "eb59be930d3848ceb242ee42096531b5",
            "cae5043563d247929d97295a423c90fb",
            "fc9c9edc6b0241878efcc4a4169f8823",
            "1d057b7c0a284d498d6857959d353feb",
            "d750eaf26aee4706910f5cb5dff97c40",
            "63bae924bd784bf2bb0654f2714870c2",
            "b44bb01ea18e4cf1a1bb8a457f68d759",
            "06ffd36456854ab68336b2fc647e3470",
            "330d20f35a6d4a21b8b34c87cd4eacca",
            "586bac86040841eeb8772bda868faa9a",
            "8dfd22bf98d84d229c0fb8fc77c1f39c",
            "07fbfce60f15431e95396e08916de24d",
            "8b3eb587eb13472e86d8756a0e023bbc",
            "c34233305b5d4161b4bf242406cde2fa",
            "30c9e8cc95de414b844a61a519c205bb",
            "c8a11d40b1474442a859169721a31317",
            "e48aa0c9b668430ab52831d016fd6dc0",
            "7de82d71aabc4cf88b64396ccbee5442",
            "79abe704f0fb4e7fb42ff0fe44aadd5a",
            "20506612975a4f57bd0383ec3fbd483e",
            "a45fe5aa3ee54a92890d1b598b84ccbc",
            "ca2658bec16a40e2a9393b9a35938fae",
            "ca2330064da149b6889b1366623064ce",
            "0c594af2ac454189be45f4e3d1d65300",
            "a8ee386e127d41f9a01204770c6ab64d",
            "de58feafb0a04b4ebf89e9e8cb8c816a",
            "8a88e29085c4456fad5a354dace331e7",
            "6345094e218743d59f6d263376fd9783",
            "86f33ef4d8194263a83585e185d0ccf0",
            "359db4d4ec7745228531e69da01269cc",
            "3cff193534a843afa2843934f908cab4",
            "b220206aa2e94dd5ae4404899396058d",
            "39c109bc05d6444f8d330eba6ec46923",
            "ac0a2a7f204d482a97f9c5eb2d5e309b"
          ]
        },
        "id": "bKu1Y4tQBcYK",
        "outputId": "c89b7539-db09-4ea6-c4e8-176025630897"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading text from: the-verdict.txt\n",
            "✓ Loaded 83 text samples from .txt file\n",
            "Total characters: 20,315\n",
            "\n",
            "Loading HuggingFace dataset: 'HuggingFaceFW/fineweb' (config: sample-10BT) (split: train)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f63d0f7b7cee427c80ea9ab26504c388"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/27468 [00:02<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cae5043563d247929d97295a423c90fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text from streaming dataset...\n",
            "  Processed 5,000 samples...\n",
            "✓ Extracted 5,000 samples from streaming dataset\n",
            "Total characters: 15,132,101\n",
            "Average text length: 3026 chars per sample\n",
            "\n",
            "Merging 2 datasets...\n",
            "  Dataset 1: 83 samples\n",
            "  Dataset 2: 5,000 samples\n",
            "Using concatenation strategy...\n",
            "✓ Merged dataset contains 5,083 total samples\n",
            "\n",
            "Creating input-target pairs (EXPLICIT SHIFTING - Custom style)...\n",
            "Max length: 128 tokens\n",
            "Stride: 4 tokens\n",
            "Tokenizing and shifting dataset...\n",
            "Using 1 worker(s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing and shifting:   0%|          | 0/5083 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b3eb587eb13472e86d8756a0e023bbc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filtering empty sequences:   0%|          | 0/453228 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c594af2ac454189be45f4e3d1d65300"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Created 453,228 input-target pairs\n",
            "Sequence length: 128 tokens (fixed)\n",
            "Total tokens: 58,013,184\n",
            "Inputs shape:  torch.Size([8, 128])\n",
            "Targets shape: torch.Size([8, 128])\n",
            "\n",
            "Inputs:\n",
            "tensor([[    1,   464,  6001,  ..., 11161,   407,   262],\n",
            "        [  465, 13476,     1,  ..., 18113,   544,  9325],\n",
            "        [ 5562,   373,   644,  ...,    11,   379,   262],\n",
            "        ...,\n",
            "        [   13, 46606,   536,  ...,   878,   402,   271],\n",
            "        [  438, 14363,   938,  ...,   338,   366, 31640],\n",
            "        [ 1650,   353,   438,  ...,    67, 20811,     1]])\n",
            "\n",
            "Targets:\n",
            "tensor([[  464,  6001,   286,  ...,   407,   262, 40123],\n",
            "        [13476,     1,   438,  ...,   544,  9325,   701],\n",
            "        [  373,   644,   262,  ...,   379,   262,   938],\n",
            "        ...,\n",
            "        [46606,   536,  5469,  ...,   402,   271, 10899],\n",
            "        [14363,   938,  4842,  ...,   366, 31640,    12],\n",
            "        [  353,   438,  2934,  ..., 20811,     1,   284]])\n",
            "\n",
            "Token embeddings shape: torch.Size([8, 128, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "id": "HeBRxXtoJS1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eadcc9f9-9c9b-48c7-80a4-38c6b43b7ea8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 128, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CREATING POSITIONAL EMBEDDINGS"
      ],
      "metadata": {
        "id": "Fgweiu1XJgIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = GPT_CONFIG_124M['max_length'] # Set the context length and max length the same\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, GPT_CONFIG_124M['emb_dim'])"
      ],
      "metadata": {
        "id": "XCyGd03ZJmzR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "id": "aChViPhYKEEj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f978ab8f-accd-4496-fa67-80212594244c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CREATE INPUT AND POSITIONAL EMBEDDING"
      ],
      "metadata": {
        "id": "AKRhyHHtMTWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gSeEO2BR9KkK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ce7b78-7d2a-414f-ece4-33080b04e31a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 128, 64])\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPLEMENTING MULTI-HEAD ATTENTION"
      ],
      "metadata": {
        "id": "9EdDvb2qKhuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "JzkTS2BoKlNp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  THE BUILDING BLOCKS-LAYER NORMALIZATION, GELU AND FEED-FORWARD NEURAL NETWORK"
      ],
      "metadata": {
        "id": "BMnE7JmCKs7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
        "            GELU(), ## Activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "KefR96x_Mg4B"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRANSFORMER BLOCK"
      ],
      "metadata": {
        "id": "MllLx5cwMkf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        # 2*4*768\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "        # 2*4*768"
      ],
      "metadata": {
        "id": "Y2Mzjp98Mmg0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  ENTIRE GPT MODEL ARCHITECTURE IMPLEMENTATION"
      ],
      "metadata": {
        "id": "cvU04SOkMpLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "aNdlsQ_wMr0h"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "ag54I0uNkDaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07f0c252-f78a-4748-9937-63e0544b3f89"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.6504, -0.2906,  0.6864,  ..., -0.1355, -0.3266,  0.5960],\n",
            "         [-0.5232,  0.8551,  0.2914,  ...,  1.4764,  0.7322,  0.2059],\n",
            "         [ 0.8078, -0.2497,  1.3213,  ...,  0.5581, -0.3751, -0.6980],\n",
            "         [-1.1853,  0.4824,  0.6537,  ..., -0.1297,  0.4060, -0.2244]],\n",
            "\n",
            "        [[-0.5507,  0.2227,  0.1802,  ...,  0.0180, -0.5139,  0.6615],\n",
            "         [-0.0905,  0.7796,  0.2680,  ...,  1.4981, -0.4316,  0.4002],\n",
            "         [ 0.4891,  0.1639,  0.4259,  ...,  0.2368, -0.2964, -0.7698],\n",
            "         [-0.7117,  0.9490,  0.4286,  ...,  0.0579,  0.1252, -0.2558]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODEL SIZE CALCULATION"
      ],
      "metadata": {
        "id": "rQ3mtUNzOqjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ],
      "metadata": {
        "id": "_YK2dftoOtTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "601757b3-fbff-40ae-f407-e928f4115f62"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 6,839,552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
        "print(\"Output layer shape:\", model.out_head.weight.shape)"
      ],
      "metadata": {
        "id": "RoFvpw57OwKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbabe6e8-672f-441e-9144-43a2741f1a48"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embedding layer shape: torch.Size([50257, 64])\n",
            "Output layer shape: torch.Size([50257, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_size_bytes = total_params * 4 #A\n",
        "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
        "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "id": "GnYi7KxROzH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb443f0-b709-4712-d878-e857b5c6d02a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the model: 26.09 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GENERATING TEXT FROM OUTPUT TOKENS - INFERENCE"
      ],
      "metadata": {
        "id": "ilgEUu2YPd7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "HQHfRAGoPjz4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"He said we came here\"\n",
        "\n",
        "\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "RTpF92YRPsWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db23b299-d42b-47a5-8bd8-904132f7135a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " He said we came here explosionsCorptimeout crust angeredbons Raymond trauma Rapp diagnosis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  CREATING TRAINING, TESTING AND VALIDATION DATA"
      ],
      "metadata": {
        "id": "0ixW94vDTE9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Split the tokenized dataset into train/validation\n",
        "train_ratio = 0.85\n",
        "split_idx = int(train_ratio * len(explicit_dataset))\n",
        "\n",
        "# Split using HuggingFace datasets\n",
        "train_dataset = explicit_dataset.select(range(split_idx))\n",
        "val_dataset = explicit_dataset.select(range(split_idx, len(explicit_dataset)))\n",
        "\n",
        "print(f\"\\nDataset split:\")\n",
        "print(f\"Training samples: {len(train_dataset):,}\")\n",
        "print(f\"Validation samples: {len(val_dataset):,}\")\n",
        "\n",
        "# Collate function\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Convert batch to tensors\"\"\"\n",
        "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
        "    labels = torch.tensor([item['labels'] for item in batch])\n",
        "    return input_ids, labels\n",
        "\n",
        "# Set manual seed for reproducibility\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Create training dataloader\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=GPT_CONFIG_124M[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# Create validation dataloader\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=GPT_CONFIG_124M[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "print(f\"\\nDataloaders created:\")\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "\n",
        "# Test iteration\n",
        "print(\"\\nTesting dataloaders...\")\n",
        "train_iter = iter(train_loader)\n",
        "inputs, targets = next(train_iter)\n",
        "print(f\"Train batch - Inputs shape: {inputs.shape}, Targets shape: {targets.shape}\")\n",
        "\n",
        "val_iter = iter(val_loader)\n",
        "inputs, targets = next(val_iter)\n",
        "print(f\"Val batch - Inputs shape: {inputs.shape}, Targets shape: {targets.shape}\")"
      ],
      "metadata": {
        "id": "s394h013THyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe85375-fe1d-4e77-e6e1-d4963cb440ea"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset split:\n",
            "Training samples: 385,243\n",
            "Validation samples: 67,985\n",
            "\n",
            "Dataloaders created:\n",
            "Training batches: 192621\n",
            "Validation batches: 33993\n",
            "\n",
            "Testing dataloaders...\n",
            "Train batch - Inputs shape: torch.Size([2, 128]), Targets shape: torch.Size([2, 128])\n",
            "Val batch - Inputs shape: torch.Size([2, 128]), Targets shape: torch.Size([2, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  DEFINING THE CROSS ENTROPY LOSS FUNCTION"
      ],
      "metadata": {
        "id": "nBgXYCsJkjlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n",
        "print(f\"Total samples: {len(train_loader) + len(val_loader)}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "2F4Z6nlJkSn6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c70cd0a0-4b30-4670-8745-1e4ed9878fa8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 192621\n",
            "Val batches: 33993\n",
            "Total samples: 226614\n",
            "Using device: cuda\n",
            "CUDA available: True\n",
            "CUDA device count: 1\n",
            "CUDA device name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "# Full calculation\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "\n",
        "# Subset Calculation\n",
        "def calc_loss_loader_subset(data_loader, model, device, num_batches=10):\n",
        "    \"\"\"Calculate loss on first num_batches only\"\"\"\n",
        "    total_loss = 0.\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets) in enumerate(data_loader):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            logits = model(inputs)\n",
        "            loss = torch.nn.functional.cross_entropy(\n",
        "                logits.flatten(0, 1), targets.flatten()\n",
        "            )\n",
        "            total_loss += loss.item()\n",
        "            count += 1\n",
        "\n",
        "    return total_loss / count if count > 0 else 0\n",
        "\n",
        "# Use subset calculation (much faster!)\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader_subset(train_loader, model, device, num_batches=10)\n",
        "    val_loss = calc_loss_loader_subset(val_loader, model, device, num_batches=10)\n",
        "\n",
        "print(f\"Training loss (first 10 batches): {train_loss}\")\n",
        "print(f\"Validation loss (first 10 batches): {val_loss}\")"
      ],
      "metadata": {
        "id": "otejnBAHUXqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09c8fdd3-5243-48af-bb20-78969ec8d091"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (first 10 batches): 10.990106773376464\n",
            "Validation loss (first 10 batches): 10.996171188354491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "id": "O5Aidl_HUq8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd195e13-1c64-4bf4-9e93-e9e7484bdd9e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CHCEK TO MAKE SURE ENOUGH DATASET FOR TRAINING"
      ],
      "metadata": {
        "id": "QsFbumswuIjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your dataset size BEFORE training\n",
        "print(f\"\\nDataset statistics:\")\n",
        "print(f\"Training samples: {len(train_loader.dataset):,}\")\n",
        "print(f\"Training batches: {len(train_loader):,}\")\n",
        "print(f\"Validation samples: {len(val_loader.dataset):,}\")\n",
        "print(f\"Validation batches: {len(val_loader):,}\")\n",
        "\n",
        "# You need AT LEAST 10,000+ samples for meaningful training\n",
        "# If you have less, load more from HuggingFace:\n",
        "if len(train_loader.dataset) < 10000:\n",
        "    print(\"\\n⚠️  WARNING: Dataset too small! Load more samples from HuggingFace\")"
      ],
      "metadata": {
        "id": "PTT_9-FSuOOO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25673e63-a8fe-4e9e-f93d-3ae5f717ab29"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset statistics:\n",
            "Training samples: 385,243\n",
            "Training batches: 192,621\n",
            "Validation samples: 67,985\n",
            "Validation batches: 33,993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRAINING LOOP FOR THE LLM"
      ],
      "metadata": {
        "id": "rYiS-aaGUtta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    \"\"\"\n",
        "    Evaluate model on train and val sets.\n",
        "    Note: Caller should handle setting model back to train mode.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    # Don't call model.train() here - let caller decide\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    \"\"\"\n",
        "    Generate and print sample text.\n",
        "    Note: Caller should handle setting model back to train mode.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "\n",
        "    try:\n",
        "        encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "        with torch.no_grad():\n",
        "            token_ids = generate_text_simple(\n",
        "                model=model, idx=encoded,\n",
        "                max_new_tokens=50, context_size=context_size\n",
        "            )\n",
        "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating sample: {e}\")"
      ],
      "metadata": {
        "id": "YUzfyxEhqiKu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer,\n",
        "                       max_grad_norm=1.0, save_checkpoints=True, checkpoint_path=\"model_checkpoint.pt\",\n",
        "                       use_amp=True, scheduler=None, gradient_accumulation_steps=1):  # ADD THIS PARAMETER\n",
        "    \"\"\"\n",
        "    Train model following OpenAI best practices.\n",
        "\n",
        "    New Args:\n",
        "        gradient_accumulation_steps: Accumulate gradients over N steps (effective batch size = batch_size * N)\n",
        "    \"\"\"\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    scaler = GradScaler() if use_amp and torch.cuda.is_available() else None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch_idx, (input_batch, target_batch) in enumerate(progress_bar):\n",
        "\n",
        "            # Forward pass with mixed precision\n",
        "            if scaler is not None:\n",
        "                with autocast():\n",
        "                    loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "                    loss = loss / gradient_accumulation_steps  # SCALE LOSS\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # Only update weights every N steps\n",
        "                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Update learning rate AFTER optimizer step\n",
        "                    if scheduler is not None:\n",
        "                        scheduler.step()\n",
        "\n",
        "                    global_step += 1\n",
        "            else:\n",
        "                # Without mixed precision\n",
        "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "                loss = loss / gradient_accumulation_steps  # SCALE LOSS\n",
        "                loss.backward()\n",
        "\n",
        "                # Only update weights every N steps\n",
        "                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    if scheduler is not None:\n",
        "                        scheduler.step()\n",
        "\n",
        "                    global_step += 1\n",
        "\n",
        "            tokens_seen += input_batch.numel()\n",
        "\n",
        "            # Update progress bar\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{(loss.item() * gradient_accumulation_steps):.3f}',  # Unscale for display\n",
        "                'lr': f'{current_lr:.2e}'\n",
        "            })\n",
        "\n",
        "            # Evaluation (only on actual gradient steps)\n",
        "            if global_step > 0 and global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "\n",
        "                print(f\"\\nEp {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}, \"\n",
        "                      f\"LR {current_lr:.2e}\")\n",
        "\n",
        "                if save_checkpoints and val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    torch.save({\n",
        "                        'epoch': epoch,\n",
        "                        'global_step': global_step,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                        'train_loss': train_loss,\n",
        "                        'val_loss': val_loss,\n",
        "                        'tokens_seen': tokens_seen,\n",
        "                    }, checkpoint_path)\n",
        "                    print(f\"✓ Saved best checkpoint (val_loss: {val_loss:.3f})\")\n",
        "\n",
        "                model.train()\n",
        "\n",
        "        # Generate sample after each epoch\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"Generated sample:\")\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "        model.train()\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "EOhXgPsiUw1S"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Set all random seeds for reproducibility\n",
        "def set_seed(seed=123):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(123)\n",
        "\n",
        "# Configuration following OpenAI best practices\n",
        "CONFIG = {\n",
        "    'num_epochs': 20,\n",
        "    'learning_rate': 3e-3,           # 0.0004\n",
        "    'weight_decay': 0.01,\n",
        "    'beta1': 0.9,                     # AdamW beta1 (OpenAI standard)\n",
        "    'beta2': 0.95,                    # AdamW beta2 (OpenAI uses 0.95 instead of default 0.999)\n",
        "    'epsilon': 1e-8,\n",
        "    'max_grad_norm': 1.0,             # Gradient clipping\n",
        "    'warmup_steps': 100,              # LR warmup steps\n",
        "    'eval_freq': 200,                 # Evaluate every N steps (not every 5 steps - too frequent)\n",
        "    'eval_iter': 50,                  # Use more batches for evaluation (not 1)\n",
        "    'use_amp': torch.cuda.is_available(),  # Automatic Mixed Precision (faster training)\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Calculate total steps\n",
        "total_steps = len(train_loader) * CONFIG['num_epochs']\n",
        "print(f\"\\nTotal steps: {total_steps}\")\n",
        "print(f\"Warmup ratio: {CONFIG['warmup_steps'] / total_steps * 100:.1f}%\")\n",
        "print(f\"Expected evaluations: {total_steps // CONFIG['eval_freq']}\")\n",
        "\n",
        "# CRITICAL FIX: Better LR scheduler\n",
        "def get_lr_scheduler_fixed(optimizer, warmup_steps, total_steps):\n",
        "    \"\"\"\n",
        "    Fixed learning rate schedule with proper warmup and cosine decay.\n",
        "    \"\"\"\n",
        "    def lr_lambda(current_step):\n",
        "        # Linear warmup\n",
        "        if current_step < warmup_steps:\n",
        "            return float(current_step) / float(max(1, warmup_steps))\n",
        "        # Cosine decay from 1.0 to 0.1\n",
        "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "        return max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "import math\n",
        "\n",
        "# Initialize model\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nModel parameters: {total_params:,}\")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=CONFIG['learning_rate'],\n",
        "    betas=(CONFIG['beta1'], CONFIG['beta2']),\n",
        "    eps=CONFIG['epsilon'],\n",
        "    weight_decay=CONFIG['weight_decay']\n",
        ")\n",
        "\n",
        "# FIXED: Correct scheduler\n",
        "scheduler = get_lr_scheduler_fixed(optimizer, CONFIG['warmup_steps'], total_steps)\n",
        "\n",
        "# Gradient scaler\n",
        "scaler = GradScaler() if CONFIG['use_amp'] else None\n",
        "\n",
        "print(f\"\\nStarting training with:\")\n",
        "print(f\"  Initial LR: {CONFIG['learning_rate']}\")\n",
        "print(f\"  After warmup: {CONFIG['learning_rate']}\")\n",
        "print(f\"  Final LR (min): {CONFIG['learning_rate'] * 0.1}\")"
      ],
      "metadata": {
        "id": "_tBMY_FQqMFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "401d9717-3b14-4025-e097-f4f4d66becfc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  num_epochs: 20\n",
            "  learning_rate: 0.003\n",
            "  weight_decay: 0.01\n",
            "  beta1: 0.9\n",
            "  beta2: 0.95\n",
            "  epsilon: 1e-08\n",
            "  max_grad_norm: 1.0\n",
            "  warmup_steps: 100\n",
            "  eval_freq: 200\n",
            "  eval_iter: 50\n",
            "  use_amp: True\n",
            "\n",
            "Total steps: 3852420\n",
            "Warmup ratio: 0.0%\n",
            "Expected evaluations: 19262\n",
            "\n",
            "Model parameters: 6,839,552\n",
            "\n",
            "Starting training with:\n",
            "  Initial LR: 0.003\n",
            "  After warmup: 0.003\n",
            "  Final LR (min): 0.00030000000000000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1131298512.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler() if CONFIG['use_amp'] else None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "# If GPU memory is limited, use gradient accumulation\n",
        "GRADIENT_ACCUMULATION_STEPS = 2  # Effective batch size = 8 * 2 = 16\n",
        "\n",
        "try:\n",
        "    # In your training code:\n",
        "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        num_epochs=CONFIG['num_epochs'],\n",
        "        eval_freq=CONFIG['eval_freq'],\n",
        "        eval_iter=CONFIG['eval_iter'],\n",
        "        start_context=\"He said we came here\",\n",
        "        tokenizer=tokenizer,\n",
        "        max_grad_norm=CONFIG['max_grad_norm'],\n",
        "        save_checkpoints=True,\n",
        "        checkpoint_path=\"gpt_model_best.pt\",\n",
        "        use_amp=CONFIG['use_amp'],\n",
        "        scheduler=scheduler,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS  # ADD THIS\n",
        "    )\n",
        "\n",
        "    # Save final model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'config': GPT_CONFIG_124M,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'tokens_seen': tokens_seen,\n",
        "    }, \"gpt_model_final.pt\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✓ Training completed successfully!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Training interrupted by user\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Save interrupted model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'config': GPT_CONFIG_124M,\n",
        "    }, \"gpt_model_interrupted.pt\")\n",
        "    print(\"✓ Saved interrupted model checkpoint\")\n",
        "\n",
        "finally:\n",
        "    end_time = time.time()\n",
        "    execution_time_minutes = (end_time - start_time) / 60\n",
        "    execution_time_hours = execution_time_minutes / 60\n",
        "\n",
        "    print(f\"\\nTraining time: {execution_time_minutes:.2f} minutes ({execution_time_hours:.2f} hours)\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Peak GPU memory: {torch.cuda.max_memory_allocated(device) / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "DmUpCwzxt80D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2aaf424-d9b3-462e-8390-69d47084ac8e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-906412445.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler() if use_amp and torch.cuda.is_available() else None\n",
            "Epoch 1/20:   0%|          | 0/192621 [00:00<?, ?it/s]/tmp/ipython-input-906412445.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 1/20:   0%|          | 401/192621 [00:18<2:14:14, 23.86it/s, loss=8.078, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 000200): Train loss 7.669, Val loss 7.961, LR 3.00e-03\n",
            "✓ Saved best checkpoint (val_loss: 7.961)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   0%|          | 407/192621 [00:20<11:07:20,  4.80it/s, loss=8.641, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 000200): Train loss 7.609, Val loss 7.961, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   0%|          | 801/192621 [00:40<2:22:54, 22.37it/s, loss=6.922, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 000400): Train loss 7.345, Val loss 7.757, LR 3.00e-03\n",
            "✓ Saved best checkpoint (val_loss: 7.757)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   0%|          | 807/192621 [00:42<11:34:03,  4.61it/s, loss=6.257, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 000400): Train loss 7.299, Val loss 7.757, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   1%|          | 1200/192621 [00:59<2:10:37, 24.42it/s, loss=7.324, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 000600): Train loss 7.251, Val loss 7.614, LR 3.00e-03\n",
            "✓ Saved best checkpoint (val_loss: 7.614)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   1%|          | 1205/192621 [01:01<12:44:21,  4.17it/s, loss=7.533, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 000600): Train loss 7.203, Val loss 7.614, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   1%|          | 1601/192621 [01:20<2:15:18, 23.53it/s, loss=7.681, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 000800): Train loss 7.113, Val loss 7.662, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   1%|          | 1604/192621 [01:21<16:44:13,  3.17it/s, loss=7.487, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 000800): Train loss 7.169, Val loss 7.662, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   1%|          | 2002/192621 [01:40<12:54:33,  4.10it/s, loss=7.281, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 001000): Train loss 7.025, Val loss 7.587, LR 3.00e-03\n",
            "✓ Saved best checkpoint (val_loss: 7.587)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   1%|          | 2007/192621 [01:42<12:14:30,  4.33it/s, loss=7.759, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 001000): Train loss 7.074, Val loss 7.587, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   1%|          | 2402/192621 [02:00<9:01:48,  5.85it/s, loss=7.440, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 001200): Train loss 7.050, Val loss 7.489, LR 3.00e-03\n",
            "✓ Saved best checkpoint (val_loss: 7.489)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   1%|          | 2407/192621 [02:01<10:18:04,  5.13it/s, loss=6.781, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 001200): Train loss 6.924, Val loss 7.489, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   1%|▏         | 2801/192621 [02:20<2:16:34, 23.17it/s, loss=7.100, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 001400): Train loss 6.927, Val loss 7.423, LR 3.00e-03\n",
            "✓ Saved best checkpoint (val_loss: 7.423)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   1%|▏         | 2807/192621 [02:22<11:28:31,  4.59it/s, loss=7.450, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 001400): Train loss 7.233, Val loss 7.423, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   2%|▏         | 3202/192621 [02:42<12:58:33,  4.05it/s, loss=7.125, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 001600): Train loss 7.005, Val loss 7.530, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   2%|▏         | 3207/192621 [02:43<12:28:56,  4.22it/s, loss=7.409, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 001600): Train loss 7.087, Val loss 7.530, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   2%|▏         | 3601/192621 [03:02<2:16:01, 23.16it/s, loss=7.655, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 001800): Train loss 6.979, Val loss 7.456, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   2%|▏         | 3607/192621 [03:03<10:47:06,  4.87it/s, loss=6.606, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 001800): Train loss 6.779, Val loss 7.456, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   2%|▏         | 4002/192621 [03:23<8:12:38,  6.38it/s, loss=7.078, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 002000): Train loss 6.879, Val loss 7.466, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   2%|▏         | 4007/192621 [03:24<9:48:12,  5.34it/s, loss=7.013, lr=3.00e-03] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 002000): Train loss 6.936, Val loss 7.466, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   2%|▏         | 4402/192621 [03:43<13:46:59,  3.79it/s, loss=6.321, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 002200): Train loss 6.783, Val loss 7.442, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   2%|▏         | 4407/192621 [03:45<12:39:30,  4.13it/s, loss=7.163, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 002200): Train loss 6.779, Val loss 7.442, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   2%|▏         | 4801/192621 [04:03<2:17:04, 22.84it/s, loss=6.882, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 002400): Train loss 6.850, Val loss 7.343, LR 3.00e-03\n",
            "✓ Saved best checkpoint (val_loss: 7.343)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   2%|▏         | 4806/192621 [04:05<13:03:21,  4.00it/s, loss=6.832, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 002400): Train loss 6.990, Val loss 7.343, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   3%|▎         | 5200/192621 [04:25<2:16:16, 22.92it/s, loss=7.169, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 002600): Train loss 6.892, Val loss 7.391, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   3%|▎         | 5206/192621 [04:26<10:51:25,  4.79it/s, loss=7.017, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 002600): Train loss 6.740, Val loss 7.391, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   3%|▎         | 5601/192621 [04:46<2:35:13, 20.08it/s, loss=7.372, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 002800): Train loss 6.804, Val loss 7.344, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   3%|▎         | 5607/192621 [04:47<11:34:23,  4.49it/s, loss=6.892, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 002800): Train loss 6.792, Val loss 7.344, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   3%|▎         | 6002/192621 [05:06<9:23:08,  5.52it/s, loss=6.079, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 003000): Train loss 6.820, Val loss 7.323, LR 3.00e-03\n",
            "✓ Saved best checkpoint (val_loss: 7.323)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   3%|▎         | 6006/192621 [05:08<12:51:43,  4.03it/s, loss=6.276, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 003000): Train loss 6.751, Val loss 7.323, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   3%|▎         | 6402/192621 [05:27<8:18:04,  6.23it/s, loss=6.063, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 003200): Train loss 6.736, Val loss 7.352, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   3%|▎         | 6407/192621 [05:28<9:45:39,  5.30it/s, loss=6.428, lr=3.00e-03] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 003200): Train loss 6.777, Val loss 7.352, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   4%|▎         | 6800/192621 [05:47<2:18:53, 22.30it/s, loss=6.806, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 003400): Train loss 6.811, Val loss 7.290, LR 3.00e-03\n",
            "✓ Saved best checkpoint (val_loss: 7.290)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   4%|▎         | 6806/192621 [05:49<11:20:19,  4.55it/s, loss=6.656, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 003400): Train loss 6.728, Val loss 7.290, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   4%|▎         | 7200/192621 [06:07<2:13:36, 23.13it/s, loss=6.259, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 003600): Train loss 6.682, Val loss 7.276, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/20:   4%|▎         | 7200/192621 [06:08<2:13:36, 23.13it/s, loss=6.276, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved best checkpoint (val_loss: 7.276)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   4%|▎         | 7205/192621 [06:10<14:37:47,  3.52it/s, loss=6.995, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 003600): Train loss 6.764, Val loss 7.276, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   4%|▍         | 7601/192621 [06:29<2:19:10, 22.16it/s, loss=6.422, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 003800): Train loss 6.792, Val loss 7.268, LR 3.00e-03\n",
            "✓ Saved best checkpoint (val_loss: 7.268)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   4%|▍         | 7607/192621 [06:31<11:11:37,  4.59it/s, loss=7.164, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 003800): Train loss 6.578, Val loss 7.268, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   4%|▍         | 8000/192621 [06:49<2:13:03, 23.13it/s, loss=7.730, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 004000): Train loss 6.687, Val loss 7.231, LR 3.00e-03\n",
            "✓ Saved best checkpoint (val_loss: 7.231)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   4%|▍         | 8006/192621 [06:52<11:09:08,  4.60it/s, loss=6.967, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 004000): Train loss 6.760, Val loss 7.231, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   4%|▍         | 8401/192621 [07:10<2:42:15, 18.92it/s, loss=6.646, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 004200): Train loss 6.711, Val loss 7.086, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/20:   4%|▍         | 8401/192621 [07:12<2:42:15, 18.92it/s, loss=8.508, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved best checkpoint (val_loss: 7.086)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   4%|▍         | 8406/192621 [07:13<13:51:22,  3.69it/s, loss=7.185, lr=3.00e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ep 1 (Step 004200): Train loss 6.747, Val loss 7.086, LR 3.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   4%|▍         | 8637/192621 [07:23<2:37:35, 19.46it/s, loss=6.570, lr=3.00e-03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Training interrupted by user\n",
            "======================================================================\n",
            "✓ Saved interrupted model checkpoint\n",
            "\n",
            "Training time: 7.40 minutes (0.12 hours)\n",
            "Peak GPU memory: 0.49 GB\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "load_txt"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f63d0f7b7cee427c80ea9ab26504c388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8bfcd604d3f242269a1d726bc23c51c7",
              "IPY_MODEL_a7832841b8284ff2bfd0011bd05973f6",
              "IPY_MODEL_52614714196d46aa82071afd69bf6cb0"
            ],
            "layout": "IPY_MODEL_bb1c863a23c446d48aa8409fa5776a32"
          }
        },
        "8bfcd604d3f242269a1d726bc23c51c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84fd5aa2ac994f31b3c139ee0f8a8c28",
            "placeholder": "​",
            "style": "IPY_MODEL_01dce4ddcc624d0d9a02ce6b80a34c9c",
            "value": "README.md: "
          }
        },
        "a7832841b8284ff2bfd0011bd05973f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9abb04fad9d34e17a9690b774b29c845",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a367c8b856c4c13b62fffe28624f5d7",
            "value": 1
          }
        },
        "52614714196d46aa82071afd69bf6cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1400f19bd79744a68cde1bcf4f171b25",
            "placeholder": "​",
            "style": "IPY_MODEL_eb59be930d3848ceb242ee42096531b5",
            "value": " 44.3k/? [00:00&lt;00:00, 1.06MB/s]"
          }
        },
        "bb1c863a23c446d48aa8409fa5776a32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84fd5aa2ac994f31b3c139ee0f8a8c28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01dce4ddcc624d0d9a02ce6b80a34c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9abb04fad9d34e17a9690b774b29c845": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2a367c8b856c4c13b62fffe28624f5d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1400f19bd79744a68cde1bcf4f171b25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb59be930d3848ceb242ee42096531b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cae5043563d247929d97295a423c90fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc9c9edc6b0241878efcc4a4169f8823",
              "IPY_MODEL_1d057b7c0a284d498d6857959d353feb",
              "IPY_MODEL_d750eaf26aee4706910f5cb5dff97c40"
            ],
            "layout": "IPY_MODEL_63bae924bd784bf2bb0654f2714870c2"
          }
        },
        "fc9c9edc6b0241878efcc4a4169f8823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b44bb01ea18e4cf1a1bb8a457f68d759",
            "placeholder": "​",
            "style": "IPY_MODEL_06ffd36456854ab68336b2fc647e3470",
            "value": "Resolving data files: 100%"
          }
        },
        "1d057b7c0a284d498d6857959d353feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_330d20f35a6d4a21b8b34c87cd4eacca",
            "max": 27468,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_586bac86040841eeb8772bda868faa9a",
            "value": 27468
          }
        },
        "d750eaf26aee4706910f5cb5dff97c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dfd22bf98d84d229c0fb8fc77c1f39c",
            "placeholder": "​",
            "style": "IPY_MODEL_07fbfce60f15431e95396e08916de24d",
            "value": " 27468/27468 [00:03&lt;00:00, 5278.96it/s]"
          }
        },
        "63bae924bd784bf2bb0654f2714870c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b44bb01ea18e4cf1a1bb8a457f68d759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06ffd36456854ab68336b2fc647e3470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "330d20f35a6d4a21b8b34c87cd4eacca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "586bac86040841eeb8772bda868faa9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8dfd22bf98d84d229c0fb8fc77c1f39c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07fbfce60f15431e95396e08916de24d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b3eb587eb13472e86d8756a0e023bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c34233305b5d4161b4bf242406cde2fa",
              "IPY_MODEL_30c9e8cc95de414b844a61a519c205bb",
              "IPY_MODEL_c8a11d40b1474442a859169721a31317"
            ],
            "layout": "IPY_MODEL_e48aa0c9b668430ab52831d016fd6dc0"
          }
        },
        "c34233305b5d4161b4bf242406cde2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7de82d71aabc4cf88b64396ccbee5442",
            "placeholder": "​",
            "style": "IPY_MODEL_79abe704f0fb4e7fb42ff0fe44aadd5a",
            "value": "Tokenizing and shifting: 100%"
          }
        },
        "30c9e8cc95de414b844a61a519c205bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20506612975a4f57bd0383ec3fbd483e",
            "max": 5083,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a45fe5aa3ee54a92890d1b598b84ccbc",
            "value": 5083
          }
        },
        "c8a11d40b1474442a859169721a31317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca2658bec16a40e2a9393b9a35938fae",
            "placeholder": "​",
            "style": "IPY_MODEL_ca2330064da149b6889b1366623064ce",
            "value": " 5083/5083 [00:35&lt;00:00, 143.00 examples/s]"
          }
        },
        "e48aa0c9b668430ab52831d016fd6dc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7de82d71aabc4cf88b64396ccbee5442": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79abe704f0fb4e7fb42ff0fe44aadd5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20506612975a4f57bd0383ec3fbd483e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a45fe5aa3ee54a92890d1b598b84ccbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca2658bec16a40e2a9393b9a35938fae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca2330064da149b6889b1366623064ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c594af2ac454189be45f4e3d1d65300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8ee386e127d41f9a01204770c6ab64d",
              "IPY_MODEL_de58feafb0a04b4ebf89e9e8cb8c816a",
              "IPY_MODEL_8a88e29085c4456fad5a354dace331e7"
            ],
            "layout": "IPY_MODEL_6345094e218743d59f6d263376fd9783"
          }
        },
        "a8ee386e127d41f9a01204770c6ab64d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86f33ef4d8194263a83585e185d0ccf0",
            "placeholder": "​",
            "style": "IPY_MODEL_359db4d4ec7745228531e69da01269cc",
            "value": "Filtering empty sequences: 100%"
          }
        },
        "de58feafb0a04b4ebf89e9e8cb8c816a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cff193534a843afa2843934f908cab4",
            "max": 453228,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b220206aa2e94dd5ae4404899396058d",
            "value": 453228
          }
        },
        "8a88e29085c4456fad5a354dace331e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39c109bc05d6444f8d330eba6ec46923",
            "placeholder": "​",
            "style": "IPY_MODEL_ac0a2a7f204d482a97f9c5eb2d5e309b",
            "value": " 453228/453228 [01:29&lt;00:00, 6125.16 examples/s]"
          }
        },
        "6345094e218743d59f6d263376fd9783": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86f33ef4d8194263a83585e185d0ccf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "359db4d4ec7745228531e69da01269cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cff193534a843afa2843934f908cab4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b220206aa2e94dd5ae4404899396058d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39c109bc05d6444f8d330eba6ec46923": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac0a2a7f204d482a97f9c5eb2d5e309b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}