{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# LLM Pre-training Dataset Preparation - Build and Train a GPT -2 Transformer model LLM From Scratch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install_cell"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers torch accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tiktoken > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "yDj-4w6vQfff"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Weights and Biases"
      ],
      "metadata": {
        "id": "9LJdmbevXNpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU"
      ],
      "metadata": {
        "id": "ERWgR5DbXM8x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODEL CONFIGURATION"
      ],
      "metadata": {
        "id": "YIOWfZKaJr99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 512, # Context length\n",
        "    \"emb_dim\": 384,         # Embedding dimension\n",
        "    \"n_heads\": 6,          # Number of attention heads\n",
        "    \"n_layers\": 6,         # Number of layers\n",
        "    \"drop_rate\": 0.2,       # Dropout rate\n",
        "    \"qkv_bias\": False,       # Query-Key-Value bias\n",
        "    \"max_length\": 512,      # Maximum sequence length\n",
        "    \"output_dimension\": 384, # Output dimension\n",
        "    \"batch_size\": 12,      # batch size\n",
        "    \"volumn_of_dataset\":10000 # Set the size of loading the dataset from HF\n",
        "}"
      ],
      "metadata": {
        "id": "sup0ja7CKyW6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "import_cell"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
        "from typing import List, Dict, Optional, Union, Tuple\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "import wandb\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjBbHxtuQjWY",
        "outputId": "5c374788-5f5a-4469-fea6-f02b996b44aa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialise W&B"
      ],
      "metadata": {
        "id": "DiwZu-3LYjXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Weights & Biases\n",
        "def init_wandb(config, project_name=\"gpt-pretraining\"):\n",
        "    \"\"\"Initialize W&B tracking\"\"\"\n",
        "    wandb.init(\n",
        "        project=project_name,\n",
        "        config=config,\n",
        "        name=f\"gpt-{config['num_epochs']}ep-lr{config['learning_rate']}\",\n",
        "        tags=[\"gpt\", \"pretraining\", \"custom\"]\n",
        "    )\n",
        "    wandb.watch(model, log=\"all\", log_freq=100)  # Log gradients and parameters"
      ],
      "metadata": {
        "id": "tUf8tN_nYmGR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_txt"
      },
      "source": [
        "## 1. Load Text from Local .txt Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "load_txt_cell"
      },
      "outputs": [],
      "source": [
        "def load_txt_file(\n",
        "    file_path: str,\n",
        "    encoding: str = 'utf-8',\n",
        "    chunk_size: Optional[int] = None,\n",
        "    overlap: int = 0\n",
        ") -> Dataset:\n",
        "    \"\"\"\n",
        "    Load text data from a .txt file and convert to HuggingFace Dataset.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the .txt file\n",
        "        encoding: Text encoding (default: 'utf-8')\n",
        "        chunk_size: Optional size to split text into chunks (in characters)\n",
        "        overlap: Number of overlapping characters between chunks\n",
        "\n",
        "    Returns:\n",
        "        HuggingFace Dataset containing text data\n",
        "    \"\"\"\n",
        "    print(f\"Loading text from: {file_path}\")\n",
        "\n",
        "    # Read the file\n",
        "    with open(file_path, 'r', encoding=encoding) as f:\n",
        "        text_content = f.read()\n",
        "\n",
        "    # Split into chunks if specified\n",
        "    if chunk_size:\n",
        "        texts = []\n",
        "        start = 0\n",
        "        while start < len(text_content):\n",
        "            end = start + chunk_size\n",
        "            texts.append(text_content[start:end])\n",
        "            start = end - overlap\n",
        "    else:\n",
        "        # Split by paragraphs (double newline) or keep as single text\n",
        "        texts = [t.strip() for t in text_content.split('\\n\\n') if t.strip()]\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = Dataset.from_dict({\"text\": texts})\n",
        "\n",
        "    print(f\"✓ Loaded {len(dataset):,} text samples from .txt file\")\n",
        "    print(f\"Total characters: {sum(len(t) for t in texts):,}\")\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_hf"
      },
      "source": [
        "## 2. Load Dataset from HuggingFace Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "load_hf_cell"
      },
      "outputs": [],
      "source": [
        "def load_huggingface_dataset(\n",
        "    dataset_name: str,\n",
        "    text_column: str = \"text\",\n",
        "    split: str = \"train\",\n",
        "    name: Optional[str] = None,\n",
        "    num_samples: Optional[int] = None,\n",
        "    streaming: bool = True,\n",
        "    trust_remote_code: bool = False\n",
        ") -> Dataset:\n",
        "    \"\"\"\n",
        "    Load dataset from HuggingFace Hub with optimized streaming support.\n",
        "\n",
        "    Args:\n",
        "        dataset_name: Name of the dataset on HuggingFace Hub\n",
        "                     (e.g., 'HuggingFaceFW/fineweb', 'openwebtext')\n",
        "        text_column: Name of the column containing text data (default: 'text')\n",
        "        split: Dataset split to load (default: 'train')\n",
        "        name: Dataset configuration name (e.g., 'sample-10BT' for fineweb)\n",
        "        num_samples: Limit number of samples to load (recommended for large datasets)\n",
        "        streaming: Use streaming mode for memory efficiency (default: True)\n",
        "        trust_remote_code: Trust remote code for custom datasets\n",
        "\n",
        "    Returns:\n",
        "        HuggingFace Dataset with 'text' column\n",
        "\n",
        "    Examples:\n",
        "        # Load FineWeb dataset\n",
        "        dataset = load_huggingface_dataset(\n",
        "            dataset_name=\"HuggingFaceFW/fineweb\",\n",
        "            name=\"sample-10BT\",\n",
        "            num_samples=10000\n",
        "        )\n",
        "\n",
        "        # Load OpenWebText\n",
        "        dataset = load_huggingface_dataset(\n",
        "            dataset_name=\"openwebtext\",\n",
        "            num_samples=5000\n",
        "        )\n",
        "    \"\"\"\n",
        "    print(f\"\\nLoading HuggingFace dataset: '{dataset_name}'\" +\n",
        "          (f\" (config: {name})\" if name else \"\") +\n",
        "          f\" (split: {split})\")\n",
        "\n",
        "    try:\n",
        "        # Load dataset with streaming for memory efficiency\n",
        "        dataset = load_dataset(\n",
        "            dataset_name,\n",
        "            name=name,\n",
        "            split=split,\n",
        "            streaming=streaming,\n",
        "            trust_remote_code=trust_remote_code\n",
        "        )\n",
        "\n",
        "        # Extract text from samples\n",
        "        texts = []\n",
        "\n",
        "        if streaming:\n",
        "            # Iterate through streaming dataset (memory efficient)\n",
        "            print(f\"Extracting text from streaming dataset...\")\n",
        "            for i, sample in enumerate(dataset):\n",
        "                if num_samples and i >= num_samples:\n",
        "                    break\n",
        "\n",
        "                # Extract text from the specified column\n",
        "                if text_column in sample:\n",
        "                    texts.append(sample[text_column])\n",
        "                else:\n",
        "                    available = list(sample.keys())\n",
        "                    raise KeyError(\n",
        "                        f\"Column '{text_column}' not found. \"\n",
        "                        f\"Available columns: {available}\"\n",
        "                    )\n",
        "\n",
        "                # Progress indicator\n",
        "                if (i + 1) % 1000 == 0:\n",
        "                    print(f\"  Processed {i + 1:,} samples...\", end=\"\\r\")\n",
        "\n",
        "            if texts:\n",
        "                print(f\"\\n✓ Extracted {len(texts):,} samples from streaming dataset\")\n",
        "        else:\n",
        "            # Non-streaming mode (loads entire dataset into memory)\n",
        "            print(f\"Loading in non-streaming mode...\")\n",
        "            if num_samples:\n",
        "                dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
        "\n",
        "            # Extract text column\n",
        "            if text_column in dataset.column_names:\n",
        "                texts = dataset[text_column]\n",
        "            else:\n",
        "                raise KeyError(\n",
        "                    f\"Column '{text_column}' not found. \"\n",
        "                    f\"Available columns: {dataset.column_names}\"\n",
        "                )\n",
        "\n",
        "            print(f\"✓ Loaded {len(texts):,} samples\")\n",
        "\n",
        "        # Create a new Dataset with only the text column\n",
        "        final_dataset = Dataset.from_dict({\"text\": texts})\n",
        "\n",
        "        total_chars = sum(len(t) for t in texts)\n",
        "        print(f\"Total characters: {total_chars:,}\")\n",
        "        print(f\"Average text length: {total_chars / len(texts):.0f} chars per sample\")\n",
        "\n",
        "        return final_dataset\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading dataset: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "merge"
      },
      "source": [
        "## 3. Merge Multiple Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "merge_cell"
      },
      "outputs": [],
      "source": [
        "def merge_datasets(\n",
        "    datasets: List[Dataset],\n",
        "    shuffle: bool = True,\n",
        "    seed: int = 42,\n",
        "    interleave: bool = False\n",
        ") -> Dataset:\n",
        "    \"\"\"\n",
        "    Merge multiple datasets into a single dataset.\n",
        "\n",
        "    Args:\n",
        "        datasets: List of HuggingFace Datasets to merge\n",
        "        shuffle: Whether to shuffle the merged dataset\n",
        "        seed: Random seed for shuffling\n",
        "        interleave: If True, interleave datasets instead of concatenating\n",
        "\n",
        "    Returns:\n",
        "        Merged HuggingFace Dataset\n",
        "    \"\"\"\n",
        "    print(f\"\\nMerging {len(datasets)} datasets...\")\n",
        "\n",
        "    # Validate all datasets have 'text' column\n",
        "    for i, ds in enumerate(datasets):\n",
        "        if 'text' not in ds.column_names:\n",
        "            raise ValueError(f\"Dataset {i} does not have 'text' column\")\n",
        "        print(f\"  Dataset {i+1}: {len(ds):,} samples\")\n",
        "\n",
        "    # Merge datasets\n",
        "    if interleave:\n",
        "        # Interleave datasets (useful for balanced sampling)\n",
        "        from datasets import interleave_datasets\n",
        "        merged_dataset = interleave_datasets(datasets, seed=seed)\n",
        "        print(\"Using interleave strategy...\")\n",
        "    else:\n",
        "        # Concatenate datasets\n",
        "        merged_dataset = concatenate_datasets(datasets)\n",
        "        print(\"Using concatenation strategy...\")\n",
        "\n",
        "    # Shuffle if requested\n",
        "    if shuffle:\n",
        "        print(\"Shuffling merged dataset...\")\n",
        "        merged_dataset = merged_dataset.shuffle(seed=seed)\n",
        "\n",
        "    print(f\"✓ Merged dataset contains {len(merged_dataset):,} total samples\")\n",
        "    return merged_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "split_explicit"
      },
      "source": [
        "### 4.2 Explicit Shifting (Custom Training Style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "explicit_cell"
      },
      "outputs": [],
      "source": [
        "def create_input_target_pairs_explicit(\n",
        "    dataset: Dataset,\n",
        "    tokenizer,\n",
        "    max_length: int = 512,\n",
        "    stride: Optional[int] = None,\n",
        "    preprocessing_num_workers: int = 1,  # DEFAULT to 1 (safer)\n",
        "    batch_size: int = 100  # REDUCED default\n",
        ") -> Dataset:\n",
        "    \"\"\"\n",
        "    Create input-target pairs for causal language modeling (EXPLICIT SHIFTING).\n",
        "    \"\"\"\n",
        "    print(f\"\\nCreating input-target pairs (EXPLICIT SHIFTING - Custom style)...\")\n",
        "    print(f\"Max length: {max_length} tokens\")\n",
        "\n",
        "    if stride is None:\n",
        "        stride = max_length\n",
        "    print(f\"Stride: {stride} tokens\")\n",
        "\n",
        "    # Warning for large datasets with multiprocessing\n",
        "    if len(dataset) > 10000 and preprocessing_num_workers > 1:\n",
        "        print(f\"⚠️  Large dataset ({len(dataset):,} samples) with multiprocessing may cause OOM\")\n",
        "        print(f\"   Recommend: preprocessing_num_workers=1\")\n",
        "\n",
        "    def tokenize_and_shift(examples):\n",
        "        \"\"\"\n",
        "        Tokenize text and create explicitly shifted input-target pairs.\n",
        "        \"\"\"\n",
        "        all_input_ids = []\n",
        "        all_labels = []\n",
        "        all_attention_mask = []\n",
        "\n",
        "        try:\n",
        "            for text in examples[\"text\"]:\n",
        "                # Skip empty texts\n",
        "                if not text or len(text.strip()) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Tokenize with tiktoken\n",
        "                try:\n",
        "                    token_ids = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Tokenization failed for text, skipping: {str(e)[:100]}\")\n",
        "                    continue\n",
        "\n",
        "                # Truncate if too long\n",
        "                if len(token_ids) > 1024:\n",
        "                    token_ids = token_ids[:1024]\n",
        "\n",
        "                # Skip if too short\n",
        "                if len(token_ids) < max_length + 1:\n",
        "                    continue\n",
        "\n",
        "                # Create sliding window chunks\n",
        "                for i in range(0, len(token_ids) - max_length, stride):\n",
        "                    input_chunk = token_ids[i : i + max_length]\n",
        "                    target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
        "\n",
        "                    # Only add if we have complete sequences\n",
        "                    if len(input_chunk) == max_length and len(target_chunk) == max_length:\n",
        "                        all_input_ids.append(input_chunk)\n",
        "                        all_labels.append(target_chunk)\n",
        "                        all_attention_mask.append([1] * max_length)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in tokenize_and_shift: {e}\")\n",
        "            # Return empty to avoid crashing\n",
        "            return {\n",
        "                \"input_ids\": [],\n",
        "                \"labels\": [],\n",
        "                \"attention_mask\": []\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": all_input_ids,\n",
        "            \"labels\": all_labels,\n",
        "            \"attention_mask\": all_attention_mask\n",
        "        }\n",
        "\n",
        "    # Apply tokenization\n",
        "    print(\"Tokenizing and shifting dataset...\")\n",
        "    print(f\"Using {preprocessing_num_workers} worker(s)\")\n",
        "\n",
        "    try:\n",
        "        tokenized_dataset = dataset.map(\n",
        "            tokenize_and_shift,\n",
        "            batched=True,\n",
        "            num_proc=preprocessing_num_workers if preprocessing_num_workers > 1 else None,\n",
        "            remove_columns=dataset.column_names,\n",
        "            batch_size=batch_size,\n",
        "            desc=\"Tokenizing and shifting\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Error during tokenization: {e}\")\n",
        "        print(\"Retrying with num_proc=1 (no multiprocessing)...\")\n",
        "\n",
        "        # Retry without multiprocessing\n",
        "        tokenized_dataset = dataset.map(\n",
        "            tokenize_and_shift,\n",
        "            batched=True,\n",
        "            num_proc=None,  # Disable multiprocessing\n",
        "            remove_columns=dataset.column_names,\n",
        "            batch_size=batch_size,\n",
        "            desc=\"Tokenizing and shifting (retry)\"\n",
        "        )\n",
        "\n",
        "    # Filter out empty sequences\n",
        "    original_len = len(tokenized_dataset)\n",
        "    tokenized_dataset = tokenized_dataset.filter(\n",
        "        lambda x: len(x['input_ids']) > 0,\n",
        "        desc=\"Filtering empty sequences\"\n",
        "    )\n",
        "\n",
        "    if len(tokenized_dataset) < original_len:\n",
        "        print(f\"Filtered out {original_len - len(tokenized_dataset)} empty sequences\")\n",
        "\n",
        "    # Calculate statistics\n",
        "    if len(tokenized_dataset) > 0:\n",
        "        print(f\"\\n✓ Created {len(tokenized_dataset):,} input-target pairs\")\n",
        "        print(f\"Sequence length: {max_length} tokens (fixed)\")\n",
        "        print(f\"Total tokens: {len(tokenized_dataset) * max_length:,}\")\n",
        "    else:\n",
        "        print(\"\\n⚠️  Warning: No sequences created! Check your data and max_length\")\n",
        "\n",
        "    return tokenized_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOAD, MERGE AND CREATING TOKEN EMBEDDINGS"
      ],
      "metadata": {
        "id": "dlJtkUtlAOUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# Load tokenizer from transformers\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "#Load tokenizer from tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Step 1: Load raw datasets (text only, no tokenization yet)\n",
        "txt_dataset = load_txt_file(\"the-verdict.txt\")\n",
        "\n",
        "hf_dataset = load_huggingface_dataset(\n",
        "    dataset_name=\"HuggingFaceFW/fineweb\",\n",
        "    name=\"sample-10BT\",\n",
        "    num_samples=GPT_CONFIG_124M['volumn_of_dataset'],\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# Step 2: Merge raw datasets (still have 'text' column)\n",
        "merged_dataset = merge_datasets(\n",
        "    datasets=[txt_dataset, hf_dataset],\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Step 3: NOW apply explicit shifting tokenization\n",
        "explicit_dataset = create_input_target_pairs_explicit(\n",
        "    dataset=merged_dataset,  # Raw text dataset\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=GPT_CONFIG_124M['max_length'],\n",
        "    stride=4\n",
        ")\n",
        "\n",
        "# Create PyTorch DataLoader\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Convert batch to tensors\"\"\"\n",
        "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
        "    labels = torch.tensor([item['labels'] for item in batch])\n",
        "    return input_ids, labels\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    explicit_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Iterate through batches\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(f\"Inputs shape:  {inputs.shape}\")\n",
        "print(f\"Targets shape: {targets.shape}\")\n",
        "print(f\"\\nInputs:\\n{inputs}\")\n",
        "print(f\"\\nTargets:\\n{targets}\")\n",
        "\n",
        "# Create token embeddings\n",
        "token_embedding_layer = torch.nn.Embedding(GPT_CONFIG_124M['vocab_size'], GPT_CONFIG_124M['emb_dim'])\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(f\"\\nToken embeddings shape: {token_embeddings.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946,
          "referenced_widgets": [
            "f5f3ce556c5249729628896d31666741",
            "11acd7574b164db3afc4d1dd0ab5e43b",
            "a6be9ed74924427e83b7dacadee0adf8",
            "08f1511afc654fc0938a5b4d93aad5c5",
            "debf562266bc4102a0efcc2ff1b1db08",
            "86cafd4b2af84374931e48cda732ae40",
            "2fc22a6a117244a3be3d8f8086f49c27",
            "7d13782e28314127bf95e016922af160",
            "7603d721621240ea840b12faf94dc9b7",
            "0614e7f9c68947d88564ff9e56fa205d",
            "556118707b1f4e21be7444e34f4c3533",
            "ffd724375a5c421c8dcb122c3791820f",
            "8eae31a0ffad4297a8f1ca71be4c4265",
            "51c56b4c1bd745a49fa6d09d18e1921a",
            "a8dfb64b29ca41daa606c83d6b100a4d",
            "f1d65dbd62de4710a2ab860b0bde84e1",
            "4233b9930b84494a8f5e5807596e92c7",
            "96d5f53a59cb4ade92d2efcf3d5aec84",
            "7c10f901d378482a9a3b9f072c445aa8",
            "6916d7ad065b432eb8adcdafec0913e6",
            "7384eb664dd747f68e503650bad7709f",
            "9c5dc49583a8451bb51b0bcd8e8a088c",
            "e5db55dfc5de456fa80a4ae0f543ec2a",
            "4e93a46e2ef24f4da0354b0b5fd79933",
            "7d2bde98ba3c4636924a00aac20a05c5",
            "2b78ce5df6a8433ea37465c2f564a437",
            "ffd6c9cac8af43489cfacb696a11c541",
            "a8be4aa067224b1d922c6fd970f8f651",
            "c5de648ee4114754ab76e82aff5c399a",
            "e4503dabb8874401924c72524fb3214f",
            "a5772c22db3a4bd99ad4399dc2efe2c8",
            "77cfb2dd1db644379f2552926b5c53d5",
            "a83079f80ebe43c08c43d905ed8bb118"
          ]
        },
        "id": "bKu1Y4tQBcYK",
        "outputId": "217ef8a2-3736-47ca-e3c9-a963dce0deea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading text from: the-verdict.txt\n",
            "✓ Loaded 83 text samples from .txt file\n",
            "Total characters: 20,315\n",
            "\n",
            "Loading HuggingFace dataset: 'HuggingFaceFW/fineweb' (config: sample-10BT) (split: train)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5f3ce556c5249729628896d31666741"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text from streaming dataset...\n",
            "  Processed 10,000 samples...\n",
            "✓ Extracted 10,000 samples from streaming dataset\n",
            "Total characters: 30,503,959\n",
            "Average text length: 3050 chars per sample\n",
            "\n",
            "Merging 2 datasets...\n",
            "  Dataset 1: 83 samples\n",
            "  Dataset 2: 10,000 samples\n",
            "Using concatenation strategy...\n",
            "✓ Merged dataset contains 10,083 total samples\n",
            "\n",
            "Creating input-target pairs (EXPLICIT SHIFTING - Custom style)...\n",
            "Max length: 512 tokens\n",
            "Stride: 4 tokens\n",
            "Tokenizing and shifting dataset...\n",
            "Using 1 worker(s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing and shifting:   0%|          | 0/10083 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffd724375a5c421c8dcb122c3791820f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filtering empty sequences:   0%|          | 0/338732 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5db55dfc5de456fa80a4ae0f543ec2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Created 338,732 input-target pairs\n",
            "Sequence length: 512 tokens (fixed)\n",
            "Total tokens: 173,430,784\n",
            "Inputs shape:  torch.Size([8, 512])\n",
            "Targets shape: torch.Size([8, 512])\n",
            "\n",
            "Inputs:\n",
            "tensor([[    9,    82,   394,  ...,   644,   339,   318],\n",
            "        [49983,   396,  2055,  ...,   546,   351,   326],\n",
            "        [ 1309,   502,  1208,  ...,   636,    11,   475],\n",
            "        ...,\n",
            "        [13366,  2569,  2055,  ...,  1281,    12,    35],\n",
            "        [  198,  1532,   345,  ...,  5855,  8845,   367],\n",
            "        [  900,  3511,   319,  ...,  5626,    39,  2751]])\n",
            "\n",
            "Targets:\n",
            "tensor([[  82,  394,    9,  ...,  339,  318, 3375],\n",
            "        [ 396, 2055,   11,  ...,  351,  326,  938],\n",
            "        [ 502, 1208,  319,  ...,   11,  475,  262],\n",
            "        ...,\n",
            "        [2569, 2055,   25,  ...,   12,   35, 2502],\n",
            "        [1532,  345,  423,  ..., 8845,  367, 2885],\n",
            "        [3511,  319, 2046,  ...,   39, 2751, 5390]])\n",
            "\n",
            "Token embeddings shape: torch.Size([8, 512, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "id": "HeBRxXtoJS1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "007f86d5-9eb2-4347-ca6c-f21d12c16b22"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 512, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CREATING POSITIONAL EMBEDDINGS"
      ],
      "metadata": {
        "id": "Fgweiu1XJgIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = GPT_CONFIG_124M['max_length'] # Set the context length and max length the same\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, GPT_CONFIG_124M['emb_dim'])"
      ],
      "metadata": {
        "id": "XCyGd03ZJmzR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "id": "aChViPhYKEEj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3269506c-7506-4aaa-9599-df576c4187ce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CREATE INPUT AND POSITIONAL EMBEDDING"
      ],
      "metadata": {
        "id": "AKRhyHHtMTWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gSeEO2BR9KkK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6281ea87-12b1-4964-ecfb-f05ea2ecffe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 512, 384])\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPLEMENTING MULTI-HEAD ATTENTION"
      ],
      "metadata": {
        "id": "9EdDvb2qKhuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "JzkTS2BoKlNp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  THE BUILDING BLOCKS-LAYER NORMALIZATION, GELU AND FEED-FORWARD NEURAL NETWORK"
      ],
      "metadata": {
        "id": "BMnE7JmCKs7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
        "            GELU(), ## Activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "KefR96x_Mg4B"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRANSFORMER BLOCK"
      ],
      "metadata": {
        "id": "MllLx5cwMkf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        # 2*4*768\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "        # 2*4*768"
      ],
      "metadata": {
        "id": "Y2Mzjp98Mmg0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  ENTIRE GPT MODEL ARCHITECTURE IMPLEMENTATION"
      ],
      "metadata": {
        "id": "cvU04SOkMpLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "aNdlsQ_wMr0h"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "ag54I0uNkDaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2740d5f-ab03-4d6d-e0ff-e043b3a2f96b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-1.0371, -0.2938,  0.5229,  ..., -0.2098,  0.7340,  1.3440],\n",
            "         [ 0.0090, -1.2032,  0.6340,  ..., -0.7390, -0.8859,  0.3126],\n",
            "         [-0.9895, -0.5937,  0.9895,  ...,  0.2455, -0.4786, -0.8173],\n",
            "         [ 0.5983,  0.1613, -0.2204,  ..., -0.6277,  0.1684,  0.1050]],\n",
            "\n",
            "        [[-0.9351, -0.3093,  0.2713,  ...,  0.7217,  0.3087,  0.6667],\n",
            "         [ 0.0657,  0.6050, -0.2442,  ..., -0.3800, -0.7365,  0.2054],\n",
            "         [ 0.6771,  0.3374,  0.9111,  ...,  1.1158, -0.8735, -0.7977],\n",
            "         [ 1.2009,  0.6100, -0.4495,  ..., -0.4903, -0.1130,  0.2402]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODEL SIZE CALCULATION"
      ],
      "metadata": {
        "id": "rQ3mtUNzOqjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ],
      "metadata": {
        "id": "_YK2dftoOtTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75fdcdd9-774e-4e9f-d102-7ab0b86befd5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 49,434,624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
        "print(\"Output layer shape:\", model.out_head.weight.shape)"
      ],
      "metadata": {
        "id": "RoFvpw57OwKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa385e7f-3112-45fc-c392-787c3dff2b05"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embedding layer shape: torch.Size([50257, 384])\n",
            "Output layer shape: torch.Size([50257, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_size_bytes = total_params * 4 #A\n",
        "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
        "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "id": "GnYi7KxROzH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acc73b68-b2c2-4f83-b472-d07c285f024c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the model: 188.58 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GENERATING TEXT FROM OUTPUT TOKENS - INFERENCE"
      ],
      "metadata": {
        "id": "ilgEUu2YPd7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "HQHfRAGoPjz4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"He said we came here\"\n",
        "\n",
        "\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "RTpF92YRPsWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8df4a39-c4d9-4d92-e9d5-4900ab98f54f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " He said we came hereLIN Kob nomine primaries� Wimscoringtersaerileen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  CREATING TRAINING, TESTING AND VALIDATION DATA"
      ],
      "metadata": {
        "id": "0ixW94vDTE9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "MAX_SEQUENCES = 60000\n",
        "if len(explicit_dataset) > MAX_SEQUENCES:\n",
        "    print(f\"⚠️  Limiting dataset: {len(explicit_dataset):,} → {MAX_SEQUENCES:,}\")\n",
        "    explicit_dataset = explicit_dataset.select(range(MAX_SEQUENCES))\n",
        "    print(f\"✅ Reduced to {len(explicit_dataset):,} sequences\")\n",
        "\n",
        "# Split the tokenized dataset into train/validation\n",
        "train_ratio = 0.85\n",
        "split_idx = int(train_ratio * len(explicit_dataset))\n",
        "\n",
        "# Split using HuggingFace datasets\n",
        "train_dataset = explicit_dataset.select(range(split_idx))\n",
        "val_dataset = explicit_dataset.select(range(split_idx, len(explicit_dataset)))\n",
        "\n",
        "print(f\"\\nDataset split:\")\n",
        "print(f\"Training samples: {len(train_dataset):,}\")\n",
        "print(f\"Validation samples: {len(val_dataset):,}\")\n",
        "\n",
        "# Collate function\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Convert batch to tensors\"\"\"\n",
        "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
        "    labels = torch.tensor([item['labels'] for item in batch])\n",
        "    return input_ids, labels\n",
        "\n",
        "# Set manual seed for reproducibility\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Create training dataloader\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=GPT_CONFIG_124M[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "# Create validation dataloader\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=GPT_CONFIG_124M[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "print(f\"\\nDataloaders created:\")\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "\n",
        "# Test iteration\n",
        "print(\"\\nTesting dataloaders...\")\n",
        "train_iter = iter(train_loader)\n",
        "inputs, targets = next(train_iter)\n",
        "print(f\"Train batch - Inputs shape: {inputs.shape}, Targets shape: {targets.shape}\")\n",
        "\n",
        "val_iter = iter(val_loader)\n",
        "inputs, targets = next(val_iter)\n",
        "print(f\"Val batch - Inputs shape: {inputs.shape}, Targets shape: {targets.shape}\")"
      ],
      "metadata": {
        "id": "s394h013THyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae559887-b3d4-438b-cbff-3c3d5e80b9e7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️  Limiting dataset: 338,732 → 60,000\n",
            "✅ Reduced to 60,000 sequences\n",
            "\n",
            "Dataset split:\n",
            "Training samples: 51,000\n",
            "Validation samples: 9,000\n",
            "\n",
            "Dataloaders created:\n",
            "Training batches: 4250\n",
            "Validation batches: 750\n",
            "\n",
            "Testing dataloaders...\n",
            "Train batch - Inputs shape: torch.Size([12, 512]), Targets shape: torch.Size([12, 512])\n",
            "Val batch - Inputs shape: torch.Size([12, 512]), Targets shape: torch.Size([12, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  DEFINING THE CROSS ENTROPY LOSS FUNCTION"
      ],
      "metadata": {
        "id": "nBgXYCsJkjlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n",
        "print(f\"Total samples: {len(train_loader) + len(val_loader)}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "2F4Z6nlJkSn6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "365033bc-1d67-4c4e-fd96-aced9032da06"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 4250\n",
            "Val batches: 750\n",
            "Total samples: 5000\n",
            "Using device: cuda\n",
            "CUDA available: True\n",
            "CUDA device count: 1\n",
            "CUDA device name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "# Full calculation\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "\n",
        "# Subset Calculation\n",
        "def calc_loss_loader_subset(data_loader, model, device, num_batches=10):\n",
        "    \"\"\"Calculate loss on first num_batches only\"\"\"\n",
        "    total_loss = 0.\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets) in enumerate(data_loader):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            logits = model(inputs)\n",
        "            loss = torch.nn.functional.cross_entropy(\n",
        "                logits.flatten(0, 1), targets.flatten()\n",
        "            )\n",
        "            total_loss += loss.item()\n",
        "            count += 1\n",
        "\n",
        "    return total_loss / count if count > 0 else 0\n",
        "\n",
        "# Use subset calculation (much faster!)\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader_subset(train_loader, model, device, num_batches=10)\n",
        "    val_loss = calc_loss_loader_subset(val_loader, model, device, num_batches=10)\n",
        "\n",
        "print(f\"Training loss (first 10 batches): {train_loss}\")\n",
        "print(f\"Validation loss (first 10 batches): {val_loss}\")"
      ],
      "metadata": {
        "id": "otejnBAHUXqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af273c9b-9f88-4333-c580-22af73a7b0b3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (first 10 batches): 10.979767608642579\n",
            "Validation loss (first 10 batches): 10.98832893371582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "id": "O5Aidl_HUq8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "753be0b4-1d48-4e92-bf97-b73dda99e67e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CHCEK TO MAKE SURE ENOUGH DATASET FOR TRAINING"
      ],
      "metadata": {
        "id": "QsFbumswuIjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your dataset size BEFORE training\n",
        "print(f\"\\nDataset statistics:\")\n",
        "print(f\"Training samples: {len(train_loader.dataset):,}\")\n",
        "print(f\"Training batches: {len(train_loader):,}\")\n",
        "print(f\"Validation samples: {len(val_loader.dataset):,}\")\n",
        "print(f\"Validation batches: {len(val_loader):,}\")\n",
        "\n",
        "# You need AT LEAST 10,000+ samples for meaningful training\n",
        "# If you have less, load more from HuggingFace:\n",
        "if len(train_loader.dataset) < 10000:\n",
        "    print(\"\\n⚠️  WARNING: Dataset too small! Load more samples from HuggingFace\")"
      ],
      "metadata": {
        "id": "PTT_9-FSuOOO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048f9788-8a5f-4ec7-dcd0-1f898a988b9f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset statistics:\n",
            "Training samples: 51,000\n",
            "Training batches: 4,250\n",
            "Validation samples: 9,000\n",
            "Validation batches: 750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRAINING LOOP FOR THE LLM"
      ],
      "metadata": {
        "id": "rYiS-aaGUtta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    \"\"\"\n",
        "    Evaluate model on train and val sets.\n",
        "    Note: Caller should handle setting model back to train mode.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    # Don't call model.train() here - let caller decide\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context,\n",
        "                              max_new_tokens=50, return_text=False):\n",
        "    \"\"\"Generate and print sample text.\"\"\"\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "\n",
        "    try:\n",
        "        encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "        with torch.no_grad():\n",
        "            token_ids = generate_text_simple(\n",
        "                model=model, idx=encoded,\n",
        "                max_new_tokens=max_new_tokens, context_size=context_size\n",
        "            )\n",
        "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "        display_text = decoded_text.replace(\"\\n\", \" \")\n",
        "        print(display_text)\n",
        "\n",
        "        if return_text:\n",
        "            return decoded_text\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error generating sample: {e}\"\n",
        "        print(error_msg)\n",
        "        if return_text:\n",
        "            return error_msg"
      ],
      "metadata": {
        "id": "YUzfyxEhqiKu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer,\n",
        "                       max_grad_norm=1.0, save_checkpoints=True, checkpoint_path=\"model_checkpoint.pt\",\n",
        "                       use_amp=True, scheduler=None, gradient_accumulation_steps=1,\n",
        "                       wandb_run=None, inference_freq=500):\n",
        "    \"\"\"\n",
        "    Train model with W&B monitoring following official best practices.\n",
        "\n",
        "    Args:\n",
        "        wandb_run: W&B run object from wandb.init()\n",
        "    \"\"\"\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    scaler = GradScaler() if use_amp and torch.cuda.is_available() else None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_train_losses = []\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch_idx, (input_batch, target_batch) in enumerate(progress_bar):\n",
        "\n",
        "            # Forward pass with mixed precision\n",
        "            if scaler is not None:\n",
        "                with autocast():\n",
        "                    loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "                    loss = loss / gradient_accumulation_steps\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    if scheduler is not None:\n",
        "                        scheduler.step()\n",
        "\n",
        "                    global_step += 1\n",
        "            else:\n",
        "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "                loss = loss / gradient_accumulation_steps\n",
        "                loss.backward()\n",
        "\n",
        "                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    if scheduler is not None:\n",
        "                        scheduler.step()\n",
        "\n",
        "                    global_step += 1\n",
        "\n",
        "            tokens_seen += input_batch.numel()\n",
        "            batch_loss = loss.item() * gradient_accumulation_steps\n",
        "            epoch_train_losses.append(batch_loss)\n",
        "\n",
        "            # Log to W&B every step using run.log() (official best practice)\n",
        "            if wandb_run and global_step > 0:\n",
        "                current_lr = optimizer.param_groups[0]['lr']\n",
        "                wandb_run.log({\n",
        "                    # Training metrics\n",
        "                    \"train/loss\": batch_loss,\n",
        "                    \"train/perplexity\": math.exp(min(batch_loss, 10)),  # Prevent overflow\n",
        "\n",
        "                    # Learning rate\n",
        "                    \"lr\": current_lr,\n",
        "\n",
        "                    # Gradient metrics\n",
        "                    \"grad_norm\": grad_norm.item() if 'grad_norm' in locals() else 0,\n",
        "\n",
        "                    # Progress metrics\n",
        "                    \"tokens_seen\": tokens_seen,\n",
        "                    \"epoch\": epoch + 1,\n",
        "\n",
        "                    # Step counter\n",
        "                    \"step\": global_step,\n",
        "                }, step=global_step)\n",
        "\n",
        "            # Update progress bar\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{batch_loss:.3f}',\n",
        "                'lr': f'{current_lr:.2e}',\n",
        "                'step': global_step\n",
        "            })\n",
        "\n",
        "            # Evaluation\n",
        "            if global_step > 0 and global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "\n",
        "                print(f\"\\nEp {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}, \"\n",
        "                      f\"LR {current_lr:.2e}\")\n",
        "\n",
        "                # Log evaluation metrics to W&B\n",
        "                if wandb_run:\n",
        "                    wandb_run.log({\n",
        "                        # Evaluation losses\n",
        "                        \"eval/train_loss\": train_loss,\n",
        "                        \"eval/val_loss\": val_loss,\n",
        "                        \"eval/loss_diff\": train_loss - val_loss,\n",
        "\n",
        "                        # Perplexity\n",
        "                        \"eval/train_perplexity\": math.exp(min(train_loss, 10)),\n",
        "                        \"eval/val_perplexity\": math.exp(min(val_loss, 10)),\n",
        "\n",
        "                        # Overfitting indicator\n",
        "                        \"eval/overfit_ratio\": val_loss / train_loss if train_loss > 0 else 1.0,\n",
        "                    }, step=global_step)\n",
        "\n",
        "\n",
        "                # ✅ ADD THIS CHECK HERE (after logging, before checkpoint)\n",
        "                if early_stopping(val_loss):\n",
        "                    print(\"\\n🛑 EARLY STOPPING - Overfitting detected!\")\n",
        "                    print(f\"Best val loss: {early_stopping.best_loss:.4f}\")\n",
        "                    break  # Exit training loop\n",
        "\n",
        "                # Save best model\n",
        "                if save_checkpoints and val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    checkpoint = {\n",
        "                        'epoch': epoch,\n",
        "                        'global_step': global_step,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "                        'train_loss': train_loss,\n",
        "                        'val_loss': val_loss,\n",
        "                        'best_val_loss': best_val_loss,\n",
        "                        'tokens_seen': tokens_seen,\n",
        "                        'config': CONFIG,\n",
        "                    }\n",
        "                    torch.save(checkpoint, checkpoint_path)\n",
        "                    print(f\"✓ Saved best checkpoint (val_loss: {val_loss:.3f})\")\n",
        "\n",
        "                    # Save checkpoint as W&B artifact\n",
        "                    if wandb_run:\n",
        "                        artifact = wandb.Artifact(\n",
        "                            name=f\"model-best\",\n",
        "                            type=\"model\",\n",
        "                            description=f\"Best model checkpoint at step {global_step}\",\n",
        "                            metadata={\n",
        "                                \"step\": global_step,\n",
        "                                \"val_loss\": val_loss,\n",
        "                                \"epoch\": epoch,\n",
        "                            }\n",
        "                        )\n",
        "                        artifact.add_file(checkpoint_path)\n",
        "                        wandb_run.log_artifact(artifact)\n",
        "\n",
        "                model.train()\n",
        "\n",
        "            # Generate inference sample\n",
        "            if global_step > 0 and global_step % inference_freq == 0:\n",
        "                print(\"\\n\" + \"=\"*70)\n",
        "                print(f\"[Step {global_step}] Generated sample:\")\n",
        "                generated_text = generate_and_print_sample(\n",
        "                    model, tokenizer, device, start_context, return_text=True\n",
        "                )\n",
        "                print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "                # Log generated text to W&B\n",
        "                if wandb_run:\n",
        "                    # Create a nice HTML table for the sample\n",
        "                    sample_html = f\"\"\"\n",
        "                    <table>\n",
        "                        <tr><th>Step</th><th>Prompt</th><th>Generated Text</th></tr>\n",
        "                        <tr>\n",
        "                            <td>{global_step}</td>\n",
        "                            <td><b>{start_context}</b></td>\n",
        "                            <td>{generated_text}</td>\n",
        "                        </tr>\n",
        "                    </table>\n",
        "                    \"\"\"\n",
        "                    wandb_run.log({\n",
        "                        \"samples/generated_text\": wandb.Html(sample_html),\n",
        "                        \"samples/text_length\": len(generated_text),\n",
        "                    }, step=global_step)\n",
        "\n",
        "                model.train()\n",
        "\n",
        "        # End of epoch - log epoch metrics\n",
        "        avg_epoch_loss = sum(epoch_train_losses) / len(epoch_train_losses) if epoch_train_losses else 0\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"End of Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Average training loss: {avg_epoch_loss:.3f}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        if wandb_run:\n",
        "            wandb_run.log({\n",
        "                \"epoch/avg_train_loss\": avg_epoch_loss,\n",
        "                \"epoch/avg_train_perplexity\": math.exp(min(avg_epoch_loss, 10)),\n",
        "                \"epoch/number\": epoch + 1,\n",
        "            }, step=global_step)\n",
        "\n",
        "        # Generate sample at end of epoch\n",
        "        print(\"End of epoch sample:\")\n",
        "        generated_text = generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context, return_text=True\n",
        "        )\n",
        "\n",
        "        if wandb_run:\n",
        "            epoch_html = f\"\"\"\n",
        "            <div style=\"padding: 10px; border: 2px solid #4CAF50; border-radius: 5px;\">\n",
        "                <h3>Epoch {epoch+1} Completion Sample</h3>\n",
        "                <p><b>Prompt:</b> {start_context}</p>\n",
        "                <p><b>Generated:</b> {generated_text}</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "            wandb_run.log({\n",
        "                \"epoch_samples/text\": wandb.Html(epoch_html),\n",
        "            }, step=global_step)\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "EOhXgPsiUw1S"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import time\n",
        "import torch\n",
        "import math\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set seed\n",
        "def set_seed(seed=123):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(123)\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    # Model architecture\n",
        "    'model_name': 'GPT-124M',\n",
        "    'vocab_size': 50257,\n",
        "    'context_length': 512,\n",
        "    'emb_dim': 384,\n",
        "    'n_heads': 6,\n",
        "    'n_layers': 6,\n",
        "    'drop_rate': 0.2,\n",
        "\n",
        "    # Training hyperparameters\n",
        "    'num_epochs': 20,\n",
        "    'batch_size': 32,\n",
        "    'learning_rate': 5e-4,\n",
        "    'weight_decay': 0.1,\n",
        "    'beta1': 0.9,\n",
        "    'beta2': 0.95,\n",
        "    'epsilon': 1e-8,\n",
        "    'max_grad_norm': 0.5,\n",
        "\n",
        "    # Learning rate schedule\n",
        "    'warmup_steps': 2000,\n",
        "    'lr_decay': 'cosine',\n",
        "\n",
        "    # Evaluation\n",
        "    'eval_freq': 1000,\n",
        "    'eval_iter': 50,\n",
        "    'inference_freq': 2000,\n",
        "\n",
        "    # Optimization\n",
        "    'use_amp': torch.cuda.is_available(),\n",
        "    'gradient_accumulation_steps': 8,\n",
        "\n",
        "    # Dataset info\n",
        "    'dataset': 'custom-text + HuggingFace/fineweb',\n",
        "    'tokenizer': 'tiktoken-gpt2',\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Early Stopping\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.1):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            return False\n",
        "\n",
        "        if val_loss > self.best_loss + self.min_delta:\n",
        "            self.counter += 1\n",
        "            print(f\"⚠️  EarlyStopping: {self.counter}/{self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        else:\n",
        "            if val_loss < self.best_loss:\n",
        "                print(f\"✅ Val improved: {self.best_loss:.4f} → {val_loss:.4f}\")\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        return False\n",
        "\n",
        "# ✅ ADD THIS INITIALIZATION\n",
        "early_stopping = EarlyStopping(patience=5, min_delta=0.1)\n",
        "print(\"✅ Early stopping ready\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize W&B run following official best practices\n",
        "run = wandb.init(\n",
        "    # Set the W&B entity (your username or team name)\n",
        "    entity=\"davidbdev-bit-labs-inc\",  # CHANGE THIS to your W&B username\n",
        "\n",
        "    # Set the W&B project where this run will be logged\n",
        "    project=\"Build LLM From Scratch\",\n",
        "\n",
        "    # Give this run a descriptive name\n",
        "    name=f\"gpt-{CONFIG['num_epochs']}ep-lr{CONFIG['learning_rate']:.0e}-bs{CONFIG['batch_size']}\",\n",
        "\n",
        "    # Add tags for easy filtering\n",
        "    tags=[\"gpt\", \"pretraining\", \"explicit-shifting\", \"tiktoken\"],\n",
        "\n",
        "    # Track hyperparameters and run metadata\n",
        "    config=CONFIG,\n",
        "\n",
        "    # Save code\n",
        "    save_code=True,\n",
        ")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"W&B Run initialized: {run.name}\")\n",
        "print(f\"W&B Run URL: {run.url}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nModel parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
        "\n",
        "# Log model info to W&B\n",
        "run.config.update({\n",
        "    \"total_params\": total_params,\n",
        "    \"trainable_params\": trainable_params,\n",
        "})\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=CONFIG['learning_rate'],\n",
        "    betas=(CONFIG['beta1'], CONFIG['beta2']),\n",
        "    eps=CONFIG['epsilon'],\n",
        "    weight_decay=CONFIG['weight_decay']\n",
        ")\n",
        "\n",
        "# LR Scheduler\n",
        "total_steps = len(train_loader) * CONFIG['num_epochs']\n",
        "\n",
        "def get_lr_scheduler_fixed(optimizer, warmup_steps, total_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < warmup_steps:\n",
        "            return float(current_step) / float(max(1, warmup_steps))\n",
        "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "        return max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "scheduler = get_lr_scheduler_fixed(optimizer, CONFIG['warmup_steps'], total_steps)\n",
        "\n",
        "# Log training info\n",
        "run.config.update({\n",
        "    \"total_steps\": total_steps,\n",
        "    \"train_batches\": len(train_loader),\n",
        "    \"val_batches\": len(val_loader),\n",
        "    \"device\": str(device),\n",
        "})\n",
        "\n",
        "\n",
        "print(f\"\\nTotal training steps: {total_steps:,}\")\n",
        "print(f\"Device: {device}\\n\")\n",
        "\n",
        "\n",
        "# Watch model in W&B (log gradients and parameters)\n",
        "run.watch(model, log=\"all\", log_freq=100)\n"
      ],
      "metadata": {
        "id": "_tBMY_FQqMFI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "be3de8ec-561c-4ccd-bd00-80bad51015be"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Early stopping ready\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavidbdev\u001b[0m (\u001b[33mdavidbdev-bit-labs-inc\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251029_102554-rykbf8mp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/davidbdev-bit-labs-inc/Build%20LLM%20From%20Scratch/runs/rykbf8mp' target=\"_blank\">gpt-20ep-lr5e-04-bs32</a></strong> to <a href='https://wandb.ai/davidbdev-bit-labs-inc/Build%20LLM%20From%20Scratch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/davidbdev-bit-labs-inc/Build%20LLM%20From%20Scratch' target=\"_blank\">https://wandb.ai/davidbdev-bit-labs-inc/Build%20LLM%20From%20Scratch</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/davidbdev-bit-labs-inc/Build%20LLM%20From%20Scratch/runs/rykbf8mp' target=\"_blank\">https://wandb.ai/davidbdev-bit-labs-inc/Build%20LLM%20From%20Scratch/runs/rykbf8mp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "W&B Run initialized: gpt-20ep-lr5e-04-bs32\n",
            "W&B Run URL: https://wandb.ai/davidbdev-bit-labs-inc/Build%20LLM%20From%20Scratch/runs/rykbf8mp\n",
            "======================================================================\n",
            "\n",
            "Model parameters: 49,434,624 total, 49,434,624 trainable\n",
            "\n",
            "Total training steps: 85,000\n",
            "Device: cuda\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start Training....."
      ],
      "metadata": {
        "id": "98sbrfIQ7lOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        num_epochs=CONFIG['num_epochs'],\n",
        "        eval_freq=CONFIG['eval_freq'],\n",
        "        eval_iter=CONFIG['eval_iter'],\n",
        "        start_context=\"He said we came here\",\n",
        "        tokenizer=tokenizer,\n",
        "        max_grad_norm=CONFIG['max_grad_norm'],\n",
        "        save_checkpoints=True,\n",
        "        checkpoint_path=\"gpt_model_best.pt\",\n",
        "        use_amp=CONFIG['use_amp'],\n",
        "        scheduler=scheduler,\n",
        "        gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
        "        wandb_run=run,  # Pass the run object\n",
        "        inference_freq=CONFIG['inference_freq']\n",
        "    )\n",
        "\n",
        "    # Save final model\n",
        "    final_checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'config': CONFIG,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'tokens_seen': tokens_seen,\n",
        "    }\n",
        "    torch.save(final_checkpoint, \"gpt_model_final.pt\")\n",
        "\n",
        "    # Save final model as W&B artifact\n",
        "    final_artifact = wandb.Artifact(\n",
        "        name=\"model-final\",\n",
        "        type=\"model\",\n",
        "        description=\"Final trained model\",\n",
        "        metadata={\n",
        "            \"epochs\": CONFIG['num_epochs'],\n",
        "            \"final_val_loss\": val_losses[-1] if val_losses else None,\n",
        "        }\n",
        "    )\n",
        "    final_artifact.add_file(\"gpt_model_final.pt\")\n",
        "    run.log_artifact(final_artifact)\n",
        "\n",
        "    # Log final summary metrics\n",
        "    run.summary.update({\n",
        "        \"final_train_loss\": train_losses[-1] if train_losses else None,\n",
        "        \"final_val_loss\": val_losses[-1] if val_losses else None,\n",
        "        \"best_val_loss\": min(val_losses) if val_losses else None,\n",
        "        \"total_tokens_seen\": tokens_seen[-1] if tokens_seen else 0,\n",
        "        \"training_time_minutes\": (time.time() - start_time) / 60,\n",
        "    })\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✓ Training completed successfully!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Training interrupted by user\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Save interrupted checkpoint\n",
        "    interrupted_checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'config': CONFIG,\n",
        "        'interrupted': True,\n",
        "    }\n",
        "    torch.save(interrupted_checkpoint, \"gpt_model_interrupted.pt\")\n",
        "    print(\"✓ Saved interrupted model checkpoint\")\n",
        "\n",
        "    # Mark run as interrupted in W&B\n",
        "    run.summary[\"interrupted\"] = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Training failed with error: {e}\")\n",
        "    run.summary[\"failed\"] = True\n",
        "    run.summary[\"error_message\"] = str(e)\n",
        "    raise\n",
        "\n",
        "finally:\n",
        "    end_time = time.time()\n",
        "    execution_time_minutes = (end_time - start_time) / 60\n",
        "    execution_time_hours = execution_time_minutes / 60\n",
        "\n",
        "    print(f\"\\nTraining time: {execution_time_minutes:.2f} minutes ({execution_time_hours:.2f} hours)\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        peak_memory_gb = torch.cuda.max_memory_allocated(device) / 1e9\n",
        "        print(f\"Peak GPU memory: {peak_memory_gb:.2f} GB\")\n",
        "        run.summary[\"peak_gpu_memory_gb\"] = peak_memory_gb\n",
        "\n",
        "    # Finish the W&B run and upload any remaining data (official best practice)\n",
        "    run.finish()\n",
        "    print(f\"\\n✓ W&B run finished: {run.url}\")"
      ],
      "metadata": {
        "id": "DmUpCwzxt80D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c2d2009-f21b-4d11-bd7a-1f74d42899ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-283499495.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler() if use_amp and torch.cuda.is_available() else None\n",
            "Epoch 1/20:   0%|          | 0/4250 [00:00<?, ?it/s]/tmp/ipython-input-283499495.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 1/20: 100%|██████████| 4250/4250 [19:21<00:00,  3.66it/s, loss=6.701, lr=1.33e-04, step=530]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "End of Epoch 1/20\n",
            "Average training loss: 8.171\n",
            "======================================================================\n",
            "\n",
            "End of epoch sample:\n",
            "He said we came here. The first time - - - - - - - - - - - - - - - - - - - - - - \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20:  88%|████████▊ | 3759/4250 [17:06<02:06,  3.89it/s, loss=5.532, lr=2.50e-04, step=1000]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ep 2 (Step 001000): Train loss 5.351, Val loss 7.242, LR 2.50e-04\n",
            "✓ Saved best checkpoint (val_loss: 7.242)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20:  88%|████████▊ | 3761/4250 [18:20<2:21:50, 17.40s/it, loss=5.322, lr=2.50e-04, step=1000]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ep 2 (Step 001000): Train loss 5.363, Val loss 7.242, LR 2.50e-04\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20:  89%|████████▊ | 3762/4250 [18:39<2:27:06, 18.09s/it, loss=5.463, lr=2.50e-04, step=1000]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ep 2 (Step 001000): Train loss 5.337, Val loss 7.242, LR 2.50e-04\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20:  89%|████████▊ | 3763/4250 [18:58<2:27:59, 18.23s/it, loss=5.608, lr=2.50e-04, step=1000]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ep 2 (Step 001000): Train loss 5.392, Val loss 7.242, LR 2.50e-04\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20:  89%|████████▊ | 3764/4250 [19:16<2:27:40, 18.23s/it, loss=5.594, lr=2.50e-04, step=1000]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ep 2 (Step 001000): Train loss 5.383, Val loss 7.242, LR 2.50e-04\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20:  89%|████████▊ | 3765/4250 [19:35<2:27:30, 18.25s/it, loss=5.468, lr=2.50e-04, step=1000]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ep 2 (Step 001000): Train loss 5.352, Val loss 7.242, LR 2.50e-04\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20:  89%|████████▊ | 3766/4250 [19:53<2:27:34, 18.29s/it, loss=5.477, lr=2.50e-04, step=1000]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ep 2 (Step 001000): Train loss 5.342, Val loss 7.242, LR 2.50e-04\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20:  89%|████████▊ | 3767/4250 [20:12<2:28:22, 18.43s/it, loss=5.461, lr=2.50e-04, step=1000]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ep 2 (Step 001000): Train loss 5.378, Val loss 7.242, LR 2.50e-04\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 4813it [24:57,  3.21it/s, loss=5.291, lr=2.83e-04, step=1131]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Training interrupted by user\n",
            "======================================================================\n",
            "✓ Saved interrupted model checkpoint\n",
            "\n",
            "Training time: 44.53 minutes (0.74 hours)\n",
            "Peak GPU memory: 6.50 GB\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "load_txt"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f5f3ce556c5249729628896d31666741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11acd7574b164db3afc4d1dd0ab5e43b",
              "IPY_MODEL_a6be9ed74924427e83b7dacadee0adf8",
              "IPY_MODEL_08f1511afc654fc0938a5b4d93aad5c5"
            ],
            "layout": "IPY_MODEL_debf562266bc4102a0efcc2ff1b1db08"
          }
        },
        "11acd7574b164db3afc4d1dd0ab5e43b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86cafd4b2af84374931e48cda732ae40",
            "placeholder": "​",
            "style": "IPY_MODEL_2fc22a6a117244a3be3d8f8086f49c27",
            "value": "Resolving data files: 100%"
          }
        },
        "a6be9ed74924427e83b7dacadee0adf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d13782e28314127bf95e016922af160",
            "max": 27468,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7603d721621240ea840b12faf94dc9b7",
            "value": 27468
          }
        },
        "08f1511afc654fc0938a5b4d93aad5c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0614e7f9c68947d88564ff9e56fa205d",
            "placeholder": "​",
            "style": "IPY_MODEL_556118707b1f4e21be7444e34f4c3533",
            "value": " 27468/27468 [00:00&lt;00:00, 42881.13it/s]"
          }
        },
        "debf562266bc4102a0efcc2ff1b1db08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86cafd4b2af84374931e48cda732ae40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fc22a6a117244a3be3d8f8086f49c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d13782e28314127bf95e016922af160": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7603d721621240ea840b12faf94dc9b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0614e7f9c68947d88564ff9e56fa205d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "556118707b1f4e21be7444e34f4c3533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffd724375a5c421c8dcb122c3791820f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8eae31a0ffad4297a8f1ca71be4c4265",
              "IPY_MODEL_51c56b4c1bd745a49fa6d09d18e1921a",
              "IPY_MODEL_a8dfb64b29ca41daa606c83d6b100a4d"
            ],
            "layout": "IPY_MODEL_f1d65dbd62de4710a2ab860b0bde84e1"
          }
        },
        "8eae31a0ffad4297a8f1ca71be4c4265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4233b9930b84494a8f5e5807596e92c7",
            "placeholder": "​",
            "style": "IPY_MODEL_96d5f53a59cb4ade92d2efcf3d5aec84",
            "value": "Tokenizing and shifting: 100%"
          }
        },
        "51c56b4c1bd745a49fa6d09d18e1921a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c10f901d378482a9a3b9f072c445aa8",
            "max": 10083,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6916d7ad065b432eb8adcdafec0913e6",
            "value": 10083
          }
        },
        "a8dfb64b29ca41daa606c83d6b100a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7384eb664dd747f68e503650bad7709f",
            "placeholder": "​",
            "style": "IPY_MODEL_9c5dc49583a8451bb51b0bcd8e8a088c",
            "value": " 10083/10083 [01:32&lt;00:00, 123.83 examples/s]"
          }
        },
        "f1d65dbd62de4710a2ab860b0bde84e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4233b9930b84494a8f5e5807596e92c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96d5f53a59cb4ade92d2efcf3d5aec84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c10f901d378482a9a3b9f072c445aa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6916d7ad065b432eb8adcdafec0913e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7384eb664dd747f68e503650bad7709f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c5dc49583a8451bb51b0bcd8e8a088c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5db55dfc5de456fa80a4ae0f543ec2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e93a46e2ef24f4da0354b0b5fd79933",
              "IPY_MODEL_7d2bde98ba3c4636924a00aac20a05c5",
              "IPY_MODEL_2b78ce5df6a8433ea37465c2f564a437"
            ],
            "layout": "IPY_MODEL_ffd6c9cac8af43489cfacb696a11c541"
          }
        },
        "4e93a46e2ef24f4da0354b0b5fd79933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8be4aa067224b1d922c6fd970f8f651",
            "placeholder": "​",
            "style": "IPY_MODEL_c5de648ee4114754ab76e82aff5c399a",
            "value": "Filtering empty sequences: 100%"
          }
        },
        "7d2bde98ba3c4636924a00aac20a05c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4503dabb8874401924c72524fb3214f",
            "max": 338732,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5772c22db3a4bd99ad4399dc2efe2c8",
            "value": 338732
          }
        },
        "2b78ce5df6a8433ea37465c2f564a437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77cfb2dd1db644379f2552926b5c53d5",
            "placeholder": "​",
            "style": "IPY_MODEL_a83079f80ebe43c08c43d905ed8bb118",
            "value": " 338732/338732 [04:06&lt;00:00, 1102.86 examples/s]"
          }
        },
        "ffd6c9cac8af43489cfacb696a11c541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8be4aa067224b1d922c6fd970f8f651": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5de648ee4114754ab76e82aff5c399a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4503dabb8874401924c72524fb3214f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5772c22db3a4bd99ad4399dc2efe2c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77cfb2dd1db644379f2552926b5c53d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a83079f80ebe43c08c43d905ed8bb118": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}